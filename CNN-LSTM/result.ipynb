{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1530359,"sourceType":"datasetVersion","datasetId":902298},{"sourceId":14003432,"sourceType":"datasetVersion","datasetId":8922312},{"sourceId":14012159,"sourceType":"datasetVersion","datasetId":8926395},{"sourceId":14012503,"sourceType":"datasetVersion","datasetId":8926670}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# clean\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport gc, warnings\nwarnings.filterwarnings('ignore')\n\nINPUT_DIR = Path('/kaggle/input/ids-intrusion-csv')      # folder chá»©a 10 CSV gá»‘c\nCLEANED_DIR = Path('/kaggle/working/cleaned_data')       # output\nCLEANED_DIR.mkdir(exist_ok=True, parents=True)\n\nCHUNK_SIZE = 500_000\n\nprint(\"=\"*70)\nprint(\"CLEAN CICIDS2018 â†’ BINARY (Benign=0, Attack=1)\")\nprint(\"=\"*70)\n\ncols_to_remove = [\n    'Src IP', 'Src_IP', 'Source IP', \n    'Dst IP', 'Dst_IP', 'Destination IP',\n    'Flow ID', 'Flow_ID',\n    'Timestamp',\n    'Src Port', 'Src_Port',\n    'Dst Port', 'Dst_Port'\n]\n\ndef standardize_columns(df):\n    df.columns = df.columns.str.strip()\n    rename_map = {\n        'Src IP': 'Src_IP',\n        'Dst IP': 'Dst_IP',\n        'Src Port': 'Src_Port',\n        'Dst Port': 'Dst_Port',\n        'Flow ID': 'Flow_ID',\n        ' Label': 'Label',\n        'Label ': 'Label',\n    }\n    df.rename(columns=rename_map, inplace=True)\n    df.columns = df.columns.str.replace(' ', '_')\n    return df\n\ndef clean_chunk(chunk):\n    chunk = standardize_columns(chunk)\n    if 'Label' not in chunk.columns:\n        return None\n    \n    # drop dup\n    chunk = chunk.drop_duplicates()\n\n    # bá» dÃ²ng quÃ¡ nhiá»u NaN\n    missing_counts = chunk.isnull().sum(axis=1)\n    chunk = chunk[missing_counts <= len(chunk.columns) * 0.3]\n\n    # fill NaN + inf cho numeric\n    num_cols = chunk.select_dtypes(include=[np.number]).columns\n    chunk.replace([np.inf, -np.inf], np.nan, inplace=True)\n    for col in num_cols:\n        if chunk[col].isnull().any():\n            chunk[col].fillna(chunk[col].median(), inplace=True)\n\n    # binary label\n    chunk['Label'] = chunk['Label'].str.strip()\n    chunk['Label_Binary'] = chunk['Label'].apply(lambda x: 0 if x == 'Benign' else 1)\n\n    # bá» cá»™t IP/port/timestamp\n    drop_cols = [c for c in cols_to_remove if c in chunk.columns]\n    if drop_cols:\n        chunk = chunk.drop(columns=drop_cols)\n\n    # thay Label gá»‘c báº±ng Label_Binary\n    chunk = chunk.drop(columns=['Label'])\n    chunk.rename(columns={'Label_Binary': 'Label'}, inplace=True)\n\n    return chunk\n\n# cháº¡y qua tá»«ng file\ncsv_files = sorted(INPUT_DIR.glob('*.csv'))\nprint(f\"Found {len(csv_files)} raw files\\n\")\n\ntotal_benign, total_attack = 0, 0\n\nfor i, fp in enumerate(csv_files, 1):\n    print(f\"[{i:>2}/{len(csv_files)}] {fp.name}\")\n    cleaned_chunks = []\n    b_cnt, a_cnt = 0, 0\n\n    for chunk in pd.read_csv(fp, chunksize=CHUNK_SIZE, low_memory=False):\n        c = clean_chunk(chunk)\n        if c is not None and len(c) > 0:\n            b_cnt += (c['Label'] == 0).sum()\n            a_cnt += (c['Label'] == 1).sum()\n            cleaned_chunks.append(c)\n        del chunk; gc.collect()\n\n    if not cleaned_chunks:\n        print(\"  â†’ no data after cleaning, skip.\")\n        continue\n\n    df_final = pd.concat(cleaned_chunks, ignore_index=True)\n    out_path = CLEANED_DIR / f\"cleaned_{fp.stem}.csv\"\n    df_final.to_csv(out_path, index=False)\n\n    print(f\"  â†’ saved {len(df_final):,} rows  (Benign: {b_cnt:,}, Attack: {a_cnt:,})\\n\")\n    total_benign += b_cnt\n    total_attack += a_cnt\n\n    del cleaned_chunks, df_final; gc.collect()\n\nprint(\"=\"*70)\nprint(\"CLEAN DONE\")\nprint(f\"Total Benign: {total_benign:,}\")\nprint(f\"Total Attack: {total_attack:,}\")\nprint(f\"Ratio: {total_benign/(total_benign+total_attack)*100:.1f}% Benign\")\nprint(f\"Output dir: {CLEANED_DIR}\")\nprint(\"=\"*70)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T12:24:39.421657Z","iopub.execute_input":"2025-12-04T12:24:39.422238Z","iopub.status.idle":"2025-12-04T12:40:51.098923Z","shell.execute_reply.started":"2025-12-04T12:24:39.422210Z","shell.execute_reply":"2025-12-04T12:40:51.098147Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"======================================================================\nCLEAN CICIDS2018 â†’ BINARY (Benign=0, Attack=1)\n======================================================================\nFound 10 raw files\n\n[ 1/10] 02-14-2018.csv\n  â†’ saved 823,447 rows  (Benign: 666,773, Attack: 156,674)\n\n[ 2/10] 02-15-2018.csv\n  â†’ saved 1,046,810 rows  (Benign: 995,070, Attack: 51,740)\n\n[ 3/10] 02-16-2018.csv\n  â†’ saved 911,350 rows  (Benign: 446,772, Attack: 464,578)\n\n[ 4/10] 02-20-2018.csv\n  â†’ saved 7,948,746 rows  (Benign: 7,372,555, Attack: 576,191)\n\n[ 5/10] 02-21-2018.csv\n  â†’ saved 1,031,019 rows  (Benign: 360,827, Attack: 670,192)\n\n[ 6/10] 02-22-2018.csv\n  â†’ saved 1,046,432 rows  (Benign: 1,046,070, Attack: 362)\n\n[ 7/10] 02-23-2018.csv\n  â†’ saved 1,046,620 rows  (Benign: 1,046,054, Attack: 566)\n\n[ 8/10] 02-28-2018.csv\n  â†’ saved 609,753 rows  (Benign: 540,892, Attack: 68,861)\n\n[ 9/10] 03-01-2018.csv\n  â†’ saved 331,028 rows  (Benign: 237,987, Attack: 93,041)\n\n[10/10] 03-02-2018.csv\n  â†’ saved 1,043,640 rows  (Benign: 761,330, Attack: 282,310)\n\n======================================================================\nCLEAN DONE\nTotal Benign: 13,474,330\nTotal Attack: 2,364,515\nRatio: 85.1% Benign\nOutput dir: /kaggle/working/cleaned_data\n======================================================================\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# import shutil\n# from pathlib import Path\n\n# base = Path(\"/kaggle/working\")\n\n# for sub in [\"kfold_splits\"]:\n#     p = base / sub\n#     if p.exists():\n#         shutil.rmtree(p)\n#         print(f\"ÄÃ£ xÃ³a: {p}\")\n#     else:\n#         print(f\"KhÃ´ng tÃ¬m tháº¥y: {p}\")\n\n!nvidia-smi","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split kfold\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport pickle, gc, warnings\nwarnings.filterwarnings('ignore')\n\nCLEANED_DIR = Path('/kaggle/working/cleaned_data')\nSPLIT_DIR = Path('/kaggle/working/kfold_splits')\nSPLIT_DIR.mkdir(exist_ok=True)\n(SPLIT_DIR / 'folds').mkdir(exist_ok=True)\n(SPLIT_DIR / 'scalers').mkdir(exist_ok=True)\n\nN_SPLITS = 5\nRANDOM_STATE = 42\nMAX_SAMPLES = 3_000_000   \n\nprint(\"=\"*70)\nprint(\"MEMORY-SAFE K-FOLD SPLIT (BINARY, FOR CNN/LSTM)\")\nprint(\"=\"*70)\n\n# 1) Scan files\ncsv_files = sorted(CLEANED_DIR.glob('cleaned_*.csv'))\nprint(f\"Found {len(csv_files)} cleaned files\\n\")\n\nfile_info = []\ntotal_rows = 0\nfor f in csv_files:\n    with open(f, 'r', encoding='utf-8', errors='ignore') as fh:\n        n_rows = sum(1 for _ in fh) - 1\n    total_rows += n_rows\n    file_info.append((f, n_rows))\n    print(f\"{f.name:30s} {n_rows:>10,} rows\")\n\nprint(f\"\\nTOTAL ROWS: {total_rows:,}\")\n\n# 2) Decide sampling ratio\nif total_rows > MAX_SAMPLES:\n    ratio = MAX_SAMPLES / total_rows\n    print(f\"\\nâš  RAM limited â†’ sampling {ratio*100:.1f}% â‰ˆ {MAX_SAMPLES:,} rows\")\nelse:\n    ratio = 1.0\n    print(\"\\nRAM OK â†’ use all rows\")\n\n# 3) Load with sampling\nall_chunks = []\nnp.random.seed(RANDOM_STATE)\ntot_b, tot_a = 0, 0\n\nfor f, n_rows in file_info:\n    n_keep = int(n_rows * ratio)\n    if n_keep < 100:\n        print(f\"\\nSkip {f.name} (too small after sampling)\")\n        continue\n    \n    print(f\"\\nLoading {f.name}: {n_keep:,}/{n_rows:,} rows...\", end=\" \")\n    if ratio < 1.0:\n        # random rows to skip\n        skip = sorted(np.random.choice(range(1, n_rows+1),\n                                       n_rows - n_keep,\n                                       replace=False))\n        df = pd.read_csv(f, skiprows=skip, low_memory=False)\n    else:\n        df = pd.read_csv(f, low_memory=False)\n    \n    b = (df['Label']==0).sum()\n    a = (df['Label']==1).sum()\n    tot_b += b\n    tot_a += a\n    all_chunks.append(df)\n    print(f\"âœ“ B:{b:,}, A:{a:,}\")\n    del df; gc.collect()\n\nprint(\"\\nMerging sampled chunks...\")\ndf = pd.concat(all_chunks, ignore_index=True)\ndel all_chunks; gc.collect()\n\nprint(\"Shuffling...\")\ndf = df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n\nprint(f\"\\nFINAL DATASET:\")\nprint(f\"  Samples: {len(df):,}\")\nprint(f\"  Benign : {tot_b:,} ({tot_b/len(df)*100:.1f}%)\")\nprint(f\"  Attack : {tot_a:,} ({tot_a/len(df)*100:.1f}%)\")\nprint(f\"  Memory : {df.memory_usage(deep=True).sum()/(1024**2):.1f} MB\")\n\n# 4) Prepare X, y\ny = df['Label'].values.astype(np.int32)\ndf_feat = df.drop(columns=['Label'])\n\ncat_cols = df_feat.select_dtypes(include=['object']).columns.tolist()\nif cat_cols:\n    print(\"\\nEncoding categorical columns:\")\n    from sklearn.preprocessing import LabelEncoder\n    for col in cat_cols:\n        le = LabelEncoder()\n        df_feat[col] = le.fit_transform(df_feat[col].astype(str))\n        print(f\"  {col} â†’ encoded\")\n\nX = df_feat.values.astype(np.float32)\nfeat_names = df_feat.columns.tolist()\n\nwith open(SPLIT_DIR / 'scalers' / 'feature_names.pkl', 'wb') as f:\n    pickle.dump(feat_names, f)\n\nprint(f\"\\nX shape: {X.shape}, y shape: {y.shape}\")\nprint(f\"X mem: {X.nbytes/(1024**2):.1f} MB\")\n\n# 5) K-Fold split\nprint(\"\\nCreating stratified K-fold splits...\")\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\nfold_meta = []\n\nfor fold, (tr_idx, te_idx) in enumerate(skf.split(X, y), 1):\n    print(f\"\\nFold {fold}/{N_SPLITS}\")\n    X_tr, X_te = X[tr_idx], X\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true,"outputs_hidden":true},"execution":{"iopub.status.busy":"2025-12-04T12:55:08.930382Z","iopub.execute_input":"2025-12-04T12:55:08.930678Z","iopub.status.idle":"2025-12-04T13:02:20.727293Z","shell.execute_reply.started":"2025-12-04T12:55:08.930654Z","shell.execute_reply":"2025-12-04T13:02:20.726382Z"},"collapsed":true},"outputs":[{"name":"stdout","text":"======================================================================\nMEMORY-SAFE K-FOLD SPLIT (BINARY, FOR CNN/LSTM)\n======================================================================\nFound 10 cleaned files\n\ncleaned_02-14-2018.csv            823,447 rows\ncleaned_02-15-2018.csv          1,046,810 rows\ncleaned_02-16-2018.csv            911,350 rows\ncleaned_02-20-2018.csv          7,948,746 rows\ncleaned_02-21-2018.csv          1,031,019 rows\ncleaned_02-22-2018.csv          1,046,432 rows\ncleaned_02-23-2018.csv          1,046,620 rows\ncleaned_02-28-2018.csv            609,753 rows\ncleaned_03-01-2018.csv            331,028 rows\ncleaned_03-02-2018.csv          1,043,640 rows\n\nTOTAL ROWS: 15,838,845\n\nâš  RAM limited â†’ sampling 18.9% â‰ˆ 3,000,000 rows\n\nLoading cleaned_02-14-2018.csv: 155,967/823,447 rows... âœ“ B:126,104, A:29,863\n\nLoading cleaned_02-15-2018.csv: 198,273/1,046,810 rows... âœ“ B:188,434, A:9,839\n\nLoading cleaned_02-16-2018.csv: 172,616/911,350 rows... âœ“ B:84,191, A:88,425\n\nLoading cleaned_02-20-2018.csv: 1,505,554/7,948,746 rows... âœ“ B:1,396,790, A:108,764\n\nLoading cleaned_02-21-2018.csv: 195,282/1,031,019 rows... âœ“ B:68,107, A:127,175\n\nLoading cleaned_02-22-2018.csv: 198,202/1,046,432 rows... âœ“ B:198,128, A:74\n\nLoading cleaned_02-23-2018.csv: 198,237/1,046,620 rows... âœ“ B:198,123, A:114\n\nLoading cleaned_02-28-2018.csv: 115,491/609,753 rows... âœ“ B:102,446, A:13,045\n\nLoading cleaned_03-01-2018.csv: 62,699/331,028 rows... âœ“ B:45,018, A:17,681\n\nLoading cleaned_03-02-2018.csv: 197,673/1,043,640 rows... âœ“ B:144,311, A:53,362\n\nMerging sampled chunks...\nShuffling...\n\nFINAL DATASET:\n  Samples: 2,999,994\n  Benign : 2,551,652 (85.1%)\n  Attack : 448,342 (14.9%)\n  Memory : 7762.9 MB\n\nEncoding categorical columns:\n  Protocol â†’ encoded\n  Flow_Duration â†’ encoded\n  Tot_Fwd_Pkts â†’ encoded\n  Tot_Bwd_Pkts â†’ encoded\n  TotLen_Fwd_Pkts â†’ encoded\n  TotLen_Bwd_Pkts â†’ encoded\n  Fwd_Pkt_Len_Max â†’ encoded\n  Fwd_Pkt_Len_Min â†’ encoded\n  Fwd_Pkt_Len_Mean â†’ encoded\n  Fwd_Pkt_Len_Std â†’ encoded\n  Bwd_Pkt_Len_Max â†’ encoded\n  Bwd_Pkt_Len_Min â†’ encoded\n  Bwd_Pkt_Len_Mean â†’ encoded\n  Bwd_Pkt_Len_Std â†’ encoded\n  Flow_Byts/s â†’ encoded\n  Flow_Pkts/s â†’ encoded\n  Flow_IAT_Mean â†’ encoded\n  Flow_IAT_Std â†’ encoded\n  Flow_IAT_Max â†’ encoded\n  Flow_IAT_Min â†’ encoded\n  Fwd_IAT_Tot â†’ encoded\n  Fwd_IAT_Mean â†’ encoded\n  Fwd_IAT_Std â†’ encoded\n  Fwd_IAT_Max â†’ encoded\n  Fwd_IAT_Min â†’ encoded\n  Bwd_IAT_Tot â†’ encoded\n  Bwd_IAT_Mean â†’ encoded\n  Bwd_IAT_Std â†’ encoded\n  Bwd_IAT_Max â†’ encoded\n  Bwd_IAT_Min â†’ encoded\n  Fwd_PSH_Flags â†’ encoded\n  Bwd_PSH_Flags â†’ encoded\n  Fwd_URG_Flags â†’ encoded\n  Bwd_URG_Flags â†’ encoded\n  Fwd_Header_Len â†’ encoded\n  Bwd_Header_Len â†’ encoded\n  Fwd_Pkts/s â†’ encoded\n  Bwd_Pkts/s â†’ encoded\n  Pkt_Len_Min â†’ encoded\n  Pkt_Len_Max â†’ encoded\n  Pkt_Len_Mean â†’ encoded\n  Pkt_Len_Std â†’ encoded\n  Pkt_Len_Var â†’ encoded\n  FIN_Flag_Cnt â†’ encoded\n  SYN_Flag_Cnt â†’ encoded\n  RST_Flag_Cnt â†’ encoded\n  PSH_Flag_Cnt â†’ encoded\n  ACK_Flag_Cnt â†’ encoded\n  URG_Flag_Cnt â†’ encoded\n  CWE_Flag_Count â†’ encoded\n  ECE_Flag_Cnt â†’ encoded\n  Down/Up_Ratio â†’ encoded\n  Pkt_Size_Avg â†’ encoded\n  Fwd_Seg_Size_Avg â†’ encoded\n  Bwd_Seg_Size_Avg â†’ encoded\n  Fwd_Byts/b_Avg â†’ encoded\n  Fwd_Pkts/b_Avg â†’ encoded\n  Fwd_Blk_Rate_Avg â†’ encoded\n  Bwd_Byts/b_Avg â†’ encoded\n  Bwd_Pkts/b_Avg â†’ encoded\n  Bwd_Blk_Rate_Avg â†’ encoded\n  Subflow_Fwd_Pkts â†’ encoded\n  Subflow_Fwd_Byts â†’ encoded\n  Subflow_Bwd_Pkts â†’ encoded\n  Subflow_Bwd_Byts â†’ encoded\n  Init_Fwd_Win_Byts â†’ encoded\n  Init_Bwd_Win_Byts â†’ encoded\n  Fwd_Act_Data_Pkts â†’ encoded\n  Fwd_Seg_Size_Min â†’ encoded\n  Active_Mean â†’ encoded\n  Active_Std â†’ encoded\n  Active_Max â†’ encoded\n  Active_Min â†’ encoded\n  Idle_Mean â†’ encoded\n  Idle_Std â†’ encoded\n  Idle_Max â†’ encoded\n  Idle_Min â†’ encoded\n\nX shape: (2999994, 77), y shape: (2999994,)\nX mem: 881.2 MB\n\nCreating stratified K-fold splits...\n\nFold 1/5\n\nFold 2/5\n\nFold 3/5\n\nFold 4/5\n\nFold 5/5\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"#  split kfold 2\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\n\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\nfold_meta = []\n\nfor fold, (tr_idx, te_idx) in enumerate(skf.split(X, y), 1):\n    print(f\"\\nFold {fold}/{N_SPLITS}\")\n    X_tr, X_te = X[tr_idx], X[te_idx]\n    y_tr, y_te = y[tr_idx], y[te_idx]\n\n    # Normalize\n    scaler = StandardScaler()\n    X_tr = scaler.fit_transform(X_tr).astype(np.float32)\n    X_te = scaler.transform(X_te).astype(np.float32)\n\n    # Save\n    fold_dir = SPLIT_DIR / 'folds' / f'fold_{fold}'\n    fold_dir.mkdir(exist_ok=True)\n\n    np.save(fold_dir / 'X_train.npy', X_tr)\n    np.save(fold_dir / 'y_train.npy', y_tr)\n    np.save(fold_dir / 'X_test.npy',  X_te)\n    np.save(fold_dir / 'y_test.npy',  y_te)\n\n    with open(fold_dir / 'scaler.pkl', 'wb') as f:\n        pickle.dump(scaler, f)\n\n    print(f\"  Saved â†’ {fold_dir}\")\n\nprint(\"\\nâœ… Split ready for CNN/LSTM training!\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true,"outputs_hidden":true},"execution":{"iopub.status.busy":"2025-12-04T13:05:16.321391Z","iopub.execute_input":"2025-12-04T13:05:16.322234Z","iopub.status.idle":"2025-12-04T13:05:45.954455Z","shell.execute_reply.started":"2025-12-04T13:05:16.322210Z","shell.execute_reply":"2025-12-04T13:05:45.953322Z"},"collapsed":true},"outputs":[{"name":"stdout","text":"\nFold 1/5\n  Saved â†’ /kaggle/working/kfold_splits/folds/fold_1\n\nFold 2/5\n  Saved â†’ /kaggle/working/kfold_splits/folds/fold_2\n\nFold 3/5\n  Saved â†’ /kaggle/working/kfold_splits/folds/fold_3\n\nFold 4/5\n  Saved â†’ /kaggle/working/kfold_splits/folds/fold_4\n\nFold 5/5\n  Saved â†’ /kaggle/working/kfold_splits/folds/fold_5\n\nâœ… Split ready for CNN/LSTM training!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# download\nimport shutil\nimport os\nfrom IPython.display import FileLink\n\n# NÃ©n toÃ n bá»™ /kaggle/working\noutput_filename = 'all_my_work'\nshutil.make_archive(output_filename, 'zip', '/kaggle/working')\n\n# Hiá»ƒn thá»‹ thÃ´ng tin\nfile_size = os.path.getsize(f'{output_filename}.zip') / (1024*1024)\nprint(f\"âœ… ÄÃ£ nÃ©n xong!\")\nprint(f\"ğŸ“¦ File: {output_filename}.zip\")\nprint(f\"ğŸ“Š KÃ­ch thÆ°á»›c: {file_size:.2f} MB\")\nprint(f\"\\nğŸ‘‡ Click link bÃªn dÆ°á»›i Ä‘á»ƒ táº£i:\")\n\n# Táº¡o link download\nFileLink(f'{output_filename}.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-04T13:08:10.325756Z","iopub.execute_input":"2025-12-04T13:08:10.326072Z","iopub.status.idle":"2025-12-04T13:15:08.478839Z","shell.execute_reply.started":"2025-12-04T13:08:10.326051Z","shell.execute_reply":"2025-12-04T13:15:08.478012Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"âœ… ÄÃ£ nÃ©n xong!\nğŸ“¦ File: all_my_work.zip\nğŸ“Š KÃ­ch thÆ°á»›c: 2997.31 MB\n\nğŸ‘‡ Click link bÃªn dÆ°á»›i Ä‘á»ƒ táº£i:\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/all_my_work.zip","text/html":"<a href='all_my_work.zip' target='_blank'>all_my_work.zip</a><br>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# train cnn\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom pathlib import Path\n\nBASE_FOLD_DIR = Path('/kaggle/working/kfold_splits/folds')\nN_FOLDS = 5\n\ndef build_cnn(input_shape):\n    model = keras.Sequential([\n        layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),\n        layers.BatchNormalization(),\n        layers.MaxPooling1D(2),\n        layers.Dropout(0.3),\n\n        layers.Conv1D(128, 3, activation='relu'),\n        layers.BatchNormalization(),\n        layers.MaxPooling1D(2),\n        layers.Dropout(0.3),\n\n        layers.Conv1D(256, 3, activation='relu'),\n        layers.BatchNormalization(),\n        layers.GlobalAveragePooling1D(),\n\n        layers.Dense(128, activation='relu'),\n        layers.Dropout(0.4),\n        layers.Dense(1, activation='sigmoid')  # Binary output\n    ])\n    model.compile(\n        optimizer=keras.optimizers.Adam(1e-3),\n        loss='binary_crossentropy',\n        metrics=[\n            'accuracy',\n            tf.keras.metrics.Precision(name='precision'),\n            tf.keras.metrics.Recall(name='recall')\n        ]\n    )\n    return model\n\nfold_results = []\n\nfor fold in range(1, N_FOLDS + 1):\n    print(\"\\n\" + \"=\"*70)\n    print(f\"TRAINING CNN - FOLD {fold}/{N_FOLDS}\")\n    print(\"=\"*70)\n\n    fold_dir = BASE_FOLD_DIR / f'fold_{fold}'\n    X_train = np.load(fold_dir / 'X_train.npy')\n    y_train = np.load(fold_dir / 'y_train.npy')\n    X_test  = np.load(fold_dir / 'X_test.npy')\n    y_test  = np.load(fold_dir / 'y_test.npy')\n\n    # Reshape for CNN: (samples, features, 1)\n    X_train_cnn = X_train.reshape(-1, X_train.shape[1], 1)\n    X_test_cnn  = X_test.reshape(-1, X_test.shape[1], 1)\n\n    print(f\"X_train: {X_train_cnn.shape}, y_train: {y_train.shape}\")\n    print(f\"X_test:  {X_test_cnn.shape}, y_test:  {y_test.shape}\")\n\n    model = build_cnn(input_shape=(X_train_cnn.shape[1], 1))\n    \n    checkpoint_path = f'/kaggle/working/cnn_fold{fold}_best.keras'\n    callbacks = [\n        keras.callbacks.ModelCheckpoint(\n            checkpoint_path, monitor='val_accuracy',\n            save_best_only=True, mode='max', verbose=1\n        ),\n        keras.callbacks.EarlyStopping(\n            monitor='val_loss', patience=5,\n            restore_best_weights=True, verbose=1\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss', factor=0.5,\n            patience=3, min_lr=1e-6, verbose=1\n        )\n    ]\n\n    history = model.fit(\n        X_train_cnn, y_train,\n        epochs=20,\n        batch_size=256,\n        validation_data=(X_test_cnn, y_test),\n        callbacks=callbacks,\n        verbose=1\n    )\n\n    # Evaluate\n    loss, acc, prec, rec = model.evaluate(X_test_cnn, y_test, verbose=0)\n    f1 = 2 * prec * rec / (prec + rec + 1e-8)\n    print(f\"\\nFOLD {fold} RESULTS:\")\n    print(f\"  Accuracy:  {acc:.4f}\")\n    print(f\"  Precision: {prec:.4f}\")\n    print(f\"  Recall:    {rec:.4f}\")\n    print(f\"  F1-score:  {f1:.4f}\")\n\n    fold_results.append({\n        'fold': fold,\n        'accuracy': acc,\n        'precision': prec,\n        'recall': rec,\n        'f1': f1\n    })\n\n# Tá»•ng há»£p káº¿t quáº£\nimport pandas as pd\n\nresults_df = pd.DataFrame(fold_results)\nprint(\"\\n\" + \"=\"*70)\nprint(\"CNN K-FOLD SUMMARY\")\nprint(\"=\"*70)\nprint(results_df)\nprint(\"\\nMEAN METRICS:\")\nprint(results_df[['accuracy','precision','recall','f1']].mean())\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true,"outputs_hidden":true},"execution":{"iopub.status.busy":"2025-12-04T13:19:01.962662Z","iopub.execute_input":"2025-12-04T13:19:01.963536Z","iopub.status.idle":"2025-12-04T14:58:51.344711Z","shell.execute_reply.started":"2025-12-04T13:19:01.963511Z","shell.execute_reply":"2025-12-04T14:58:51.343996Z"},"collapsed":true},"outputs":[{"name":"stdout","text":"\n======================================================================\nTRAINING CNN - FOLD 1/5\n======================================================================\nX_train: (2399995, 77, 1), y_train: (2399995,)\nX_test:  (599999, 77, 1), y_test:  (599999,)\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1764854344.811057      47 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1764854344.811704      47 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1764854351.693041     276 service.cc:148] XLA service 0x7cd39802f1e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1764854351.693720     276 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1764854351.693740     276 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1764854352.241225     276 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  19/9375\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m1:20\u001b[0m 9ms/step - accuracy: 0.7062 - loss: 0.5475 - precision: 0.1839 - recall: 0.2647 ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1764854357.056702     276 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9688 - loss: 0.0971 - precision: 0.9345 - recall: 0.8495\nEpoch 1: val_accuracy improved from -inf to 0.98761, saving model to /kaggle/working/cnn_fold1_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 7ms/step - accuracy: 0.9688 - loss: 0.0971 - precision: 0.9345 - recall: 0.8495 - val_accuracy: 0.9876 - val_loss: 0.0501 - val_precision: 0.9857 - val_recall: 0.9306 - learning_rate: 0.0010\nEpoch 2/20\n\u001b[1m9367/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9871 - loss: 0.0528 - precision: 0.9913 - recall: 0.9218\nEpoch 2: val_accuracy did not improve from 0.98761\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - accuracy: 0.9871 - loss: 0.0528 - precision: 0.9913 - recall: 0.9218 - val_accuracy: 0.9875 - val_loss: 0.0491 - val_precision: 0.9933 - val_recall: 0.9224 - learning_rate: 0.0010\nEpoch 3/20\n\u001b[1m9367/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9874 - loss: 0.0502 - precision: 0.9925 - recall: 0.9229\nEpoch 3: val_accuracy improved from 0.98761 to 0.98811, saving model to /kaggle/working/cnn_fold1_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - accuracy: 0.9874 - loss: 0.0502 - precision: 0.9925 - recall: 0.9229 - val_accuracy: 0.9881 - val_loss: 0.0458 - val_precision: 0.9939 - val_recall: 0.9261 - learning_rate: 0.0010\nEpoch 4/20\n\u001b[1m9373/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9874 - loss: 0.0490 - precision: 0.9931 - recall: 0.9223\nEpoch 4: val_accuracy improved from 0.98811 to 0.98818, saving model to /kaggle/working/cnn_fold1_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - accuracy: 0.9874 - loss: 0.0490 - precision: 0.9931 - recall: 0.9223 - val_accuracy: 0.9882 - val_loss: 0.0450 - val_precision: 0.9943 - val_recall: 0.9262 - learning_rate: 0.0010\nEpoch 5/20\n\u001b[1m9367/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9876 - loss: 0.0478 - precision: 0.9930 - recall: 0.9233\nEpoch 5: val_accuracy improved from 0.98818 to 0.98824, saving model to /kaggle/working/cnn_fold1_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - accuracy: 0.9876 - loss: 0.0478 - precision: 0.9930 - recall: 0.9233 - val_accuracy: 0.9882 - val_loss: 0.0447 - val_precision: 0.9944 - val_recall: 0.9266 - learning_rate: 0.0010\nEpoch 6/20\n\u001b[1m9372/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9877 - loss: 0.0472 - precision: 0.9932 - recall: 0.9238\nEpoch 6: val_accuracy improved from 0.98824 to 0.98833, saving model to /kaggle/working/cnn_fold1_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - accuracy: 0.9877 - loss: 0.0472 - precision: 0.9932 - recall: 0.9238 - val_accuracy: 0.9883 - val_loss: 0.0438 - val_precision: 0.9927 - val_recall: 0.9287 - learning_rate: 0.0010\nEpoch 7/20\n\u001b[1m9370/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0461 - precision: 0.9933 - recall: 0.9252\nEpoch 7: val_accuracy did not improve from 0.98833\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0461 - precision: 0.9933 - recall: 0.9252 - val_accuracy: 0.9882 - val_loss: 0.0444 - val_precision: 0.9937 - val_recall: 0.9271 - learning_rate: 0.0010\nEpoch 8/20\n\u001b[1m9370/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0459 - precision: 0.9935 - recall: 0.9249\nEpoch 8: val_accuracy did not improve from 0.98833\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0459 - precision: 0.9935 - recall: 0.9249 - val_accuracy: 0.9880 - val_loss: 0.0435 - val_precision: 0.9864 - val_recall: 0.9328 - learning_rate: 0.0010\nEpoch 9/20\n\u001b[1m9371/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9878 - loss: 0.0460 - precision: 0.9933 - recall: 0.9247\nEpoch 9: val_accuracy did not improve from 0.98833\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - accuracy: 0.9878 - loss: 0.0460 - precision: 0.9933 - recall: 0.9247 - val_accuracy: 0.9881 - val_loss: 0.0433 - val_precision: 0.9861 - val_recall: 0.9336 - learning_rate: 0.0010\nEpoch 10/20\n\u001b[1m9371/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0456 - precision: 0.9935 - recall: 0.9246\nEpoch 10: val_accuracy improved from 0.98833 to 0.98845, saving model to /kaggle/working/cnn_fold1_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0456 - precision: 0.9935 - recall: 0.9246 - val_accuracy: 0.9884 - val_loss: 0.0430 - val_precision: 0.9934 - val_recall: 0.9288 - learning_rate: 0.0010\nEpoch 11/20\n\u001b[1m9373/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9877 - loss: 0.0458 - precision: 0.9935 - recall: 0.9239\nEpoch 11: val_accuracy improved from 0.98845 to 0.98849, saving model to /kaggle/working/cnn_fold1_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9877 - loss: 0.0458 - precision: 0.9935 - recall: 0.9239 - val_accuracy: 0.9885 - val_loss: 0.0430 - val_precision: 0.9957 - val_recall: 0.9270 - learning_rate: 0.0010\nEpoch 12/20\n\u001b[1m9374/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0453 - precision: 0.9937 - recall: 0.9247\nEpoch 12: val_accuracy did not improve from 0.98849\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0453 - precision: 0.9937 - recall: 0.9247 - val_accuracy: 0.9559 - val_loss: 0.0954 - val_precision: 0.9931 - val_recall: 0.7099 - learning_rate: 0.0010\nEpoch 13/20\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0449 - precision: 0.9940 - recall: 0.9255\nEpoch 13: val_accuracy did not improve from 0.98849\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9880 - loss: 0.0449 - precision: 0.9940 - recall: 0.9255 - val_accuracy: 0.9884 - val_loss: 0.0424 - val_precision: 0.9953 - val_recall: 0.9271 - learning_rate: 0.0010\nEpoch 14/20\n\u001b[1m9373/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9877 - loss: 0.0453 - precision: 0.9934 - recall: 0.9243\nEpoch 14: val_accuracy did not improve from 0.98849\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - accuracy: 0.9877 - loss: 0.0453 - precision: 0.9934 - recall: 0.9243 - val_accuracy: 0.9884 - val_loss: 0.0425 - val_precision: 0.9957 - val_recall: 0.9266 - learning_rate: 0.0010\nEpoch 15/20\n\u001b[1m9373/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0451 - precision: 0.9943 - recall: 0.9242\nEpoch 15: val_accuracy did not improve from 0.98849\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0451 - precision: 0.9943 - recall: 0.9242 - val_accuracy: 0.9882 - val_loss: 0.0428 - val_precision: 0.9864 - val_recall: 0.9336 - learning_rate: 0.0010\nEpoch 16/20\n\u001b[1m9371/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0449 - precision: 0.9934 - recall: 0.9251\nEpoch 16: val_accuracy did not improve from 0.98849\n\nEpoch 16: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0449 - precision: 0.9934 - recall: 0.9251 - val_accuracy: 0.9883 - val_loss: 0.0432 - val_precision: 0.9895 - val_recall: 0.9314 - learning_rate: 0.0010\nEpoch 17/20\n\u001b[1m9373/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0442 - precision: 0.9942 - recall: 0.9252\nEpoch 17: val_accuracy did not improve from 0.98849\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - accuracy: 0.9880 - loss: 0.0442 - precision: 0.9942 - recall: 0.9252 - val_accuracy: 0.9884 - val_loss: 0.0419 - val_precision: 0.9913 - val_recall: 0.9309 - learning_rate: 5.0000e-04\nEpoch 18/20\n\u001b[1m9371/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9882 - loss: 0.0434 - precision: 0.9945 - recall: 0.9260\nEpoch 18: val_accuracy improved from 0.98849 to 0.98853, saving model to /kaggle/working/cnn_fold1_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - accuracy: 0.9882 - loss: 0.0434 - precision: 0.9945 - recall: 0.9260 - val_accuracy: 0.9885 - val_loss: 0.0419 - val_precision: 0.9952 - val_recall: 0.9277 - learning_rate: 5.0000e-04\nEpoch 19/20\n\u001b[1m9373/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9882 - loss: 0.0430 - precision: 0.9946 - recall: 0.9263\nEpoch 19: val_accuracy did not improve from 0.98853\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9882 - loss: 0.0430 - precision: 0.9946 - recall: 0.9263 - val_accuracy: 0.9884 - val_loss: 0.0414 - val_precision: 0.9900 - val_recall: 0.9320 - learning_rate: 5.0000e-04\nEpoch 20/20\n\u001b[1m9370/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9881 - loss: 0.0433 - precision: 0.9944 - recall: 0.9257\nEpoch 20: val_accuracy did not improve from 0.98853\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - accuracy: 0.9881 - loss: 0.0433 - precision: 0.9944 - recall: 0.9257 - val_accuracy: 0.9882 - val_loss: 0.0418 - val_precision: 0.9846 - val_recall: 0.9354 - learning_rate: 5.0000e-04\nRestoring model weights from the end of the best epoch: 19.\n\nFOLD 1 RESULTS:\n  Accuracy:  0.9884\n  Precision: 0.9900\n  Recall:    0.9320\n  F1-score:  0.9601\n\n======================================================================\nTRAINING CNN - FOLD 2/5\n======================================================================\nX_train: (2399995, 77, 1), y_train: (2399995,)\nX_test:  (599999, 77, 1), y_test:  (599999,)\nEpoch 1/20\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9690 - loss: 0.0969 - precision: 0.9348 - recall: 0.8511\nEpoch 1: val_accuracy improved from -inf to 0.98741, saving model to /kaggle/working/cnn_fold2_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 7ms/step - accuracy: 0.9690 - loss: 0.0969 - precision: 0.9348 - recall: 0.8511 - val_accuracy: 0.9874 - val_loss: 0.0494 - val_precision: 0.9898 - val_recall: 0.9253 - learning_rate: 0.0010\nEpoch 2/20\n\u001b[1m9370/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9870 - loss: 0.0528 - precision: 0.9911 - recall: 0.9215\nEpoch 2: val_accuracy improved from 0.98741 to 0.98741, saving model to /kaggle/working/cnn_fold2_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9870 - loss: 0.0528 - precision: 0.9911 - recall: 0.9215 - val_accuracy: 0.9874 - val_loss: 0.0488 - val_precision: 0.9901 - val_recall: 0.9250 - learning_rate: 0.0010\nEpoch 3/20\n\u001b[1m9371/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9873 - loss: 0.0502 - precision: 0.9923 - recall: 0.9225\nEpoch 3: val_accuracy improved from 0.98741 to 0.98775, saving model to /kaggle/working/cnn_fold2_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9873 - loss: 0.0502 - precision: 0.9923 - recall: 0.9225 - val_accuracy: 0.9877 - val_loss: 0.0460 - val_precision: 0.9869 - val_recall: 0.9304 - learning_rate: 0.0010\nEpoch 4/20\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9875 - loss: 0.0487 - precision: 0.9924 - recall: 0.9234\nEpoch 4: val_accuracy improved from 0.98775 to 0.98797, saving model to /kaggle/working/cnn_fold2_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9875 - loss: 0.0487 - precision: 0.9924 - recall: 0.9234 - val_accuracy: 0.9880 - val_loss: 0.0455 - val_precision: 0.9940 - val_recall: 0.9251 - learning_rate: 0.0010\nEpoch 5/20\n\u001b[1m9368/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9878 - loss: 0.0477 - precision: 0.9928 - recall: 0.9250\nEpoch 5: val_accuracy did not improve from 0.98797\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9878 - loss: 0.0477 - precision: 0.9928 - recall: 0.9249 - val_accuracy: 0.9875 - val_loss: 0.0451 - val_precision: 0.9826 - val_recall: 0.9327 - learning_rate: 0.0010\nEpoch 6/20\n\u001b[1m9374/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9878 - loss: 0.0469 - precision: 0.9930 - recall: 0.9251\nEpoch 6: val_accuracy did not improve from 0.98797\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9878 - loss: 0.0469 - precision: 0.9930 - recall: 0.9251 - val_accuracy: 0.9879 - val_loss: 0.0440 - val_precision: 0.9871 - val_recall: 0.9314 - learning_rate: 0.0010\nEpoch 7/20\n\u001b[1m9369/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9878 - loss: 0.0467 - precision: 0.9929 - recall: 0.9249\nEpoch 7: val_accuracy improved from 0.98797 to 0.98812, saving model to /kaggle/working/cnn_fold2_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9878 - loss: 0.0467 - precision: 0.9929 - recall: 0.9249 - val_accuracy: 0.9881 - val_loss: 0.0438 - val_precision: 0.9906 - val_recall: 0.9293 - learning_rate: 0.0010\nEpoch 8/20\n\u001b[1m9366/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9877 - loss: 0.0464 - precision: 0.9927 - recall: 0.9247\nEpoch 8: val_accuracy improved from 0.98812 to 0.98829, saving model to /kaggle/working/cnn_fold2_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9877 - loss: 0.0464 - precision: 0.9927 - recall: 0.9247 - val_accuracy: 0.9883 - val_loss: 0.0433 - val_precision: 0.9967 - val_recall: 0.9248 - learning_rate: 0.0010\nEpoch 9/20\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0459 - precision: 0.9935 - recall: 0.9250\nEpoch 9: val_accuracy did not improve from 0.98829\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0459 - precision: 0.9935 - recall: 0.9250 - val_accuracy: 0.9881 - val_loss: 0.0433 - val_precision: 0.9919 - val_recall: 0.9281 - learning_rate: 0.0010\nEpoch 10/20\n\u001b[1m9370/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9878 - loss: 0.0459 - precision: 0.9931 - recall: 0.9248\nEpoch 10: val_accuracy did not improve from 0.98829\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9878 - loss: 0.0459 - precision: 0.9931 - recall: 0.9248 - val_accuracy: 0.9880 - val_loss: 0.0435 - val_precision: 0.9861 - val_recall: 0.9327 - learning_rate: 0.0010\nEpoch 11/20\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0456 - precision: 0.9928 - recall: 0.9256\nEpoch 11: val_accuracy did not improve from 0.98829\n\nEpoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0456 - precision: 0.9928 - recall: 0.9256 - val_accuracy: 0.9879 - val_loss: 0.0433 - val_precision: 0.9855 - val_recall: 0.9331 - learning_rate: 0.0010\nEpoch 12/20\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0442 - precision: 0.9938 - recall: 0.9254\nEpoch 12: val_accuracy did not improve from 0.98829\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9880 - loss: 0.0442 - precision: 0.9938 - recall: 0.9254 - val_accuracy: 0.9883 - val_loss: 0.0419 - val_precision: 0.9911 - val_recall: 0.9298 - learning_rate: 5.0000e-04\nEpoch 13/20\n\u001b[1m9366/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9881 - loss: 0.0437 - precision: 0.9939 - recall: 0.9260\nEpoch 13: val_accuracy did not improve from 0.98829\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9881 - loss: 0.0437 - precision: 0.9939 - recall: 0.9260 - val_accuracy: 0.9881 - val_loss: 0.0418 - val_precision: 0.9876 - val_recall: 0.9324 - learning_rate: 5.0000e-04\nEpoch 14/20\n\u001b[1m9367/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9881 - loss: 0.0437 - precision: 0.9938 - recall: 0.9262\nEpoch 14: val_accuracy improved from 0.98829 to 0.98838, saving model to /kaggle/working/cnn_fold2_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9881 - loss: 0.0437 - precision: 0.9938 - recall: 0.9262 - val_accuracy: 0.9884 - val_loss: 0.0413 - val_precision: 0.9942 - val_recall: 0.9276 - learning_rate: 5.0000e-04\nEpoch 15/20\n\u001b[1m9373/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9881 - loss: 0.0433 - precision: 0.9939 - recall: 0.9263\nEpoch 15: val_accuracy did not improve from 0.98838\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9881 - loss: 0.0433 - precision: 0.9939 - recall: 0.9263 - val_accuracy: 0.9882 - val_loss: 0.0413 - val_precision: 0.9908 - val_recall: 0.9298 - learning_rate: 5.0000e-04\nEpoch 16/20\n\u001b[1m9374/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9881 - loss: 0.0432 - precision: 0.9939 - recall: 0.9259\nEpoch 16: val_accuracy improved from 0.98838 to 0.98838, saving model to /kaggle/working/cnn_fold2_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9881 - loss: 0.0432 - precision: 0.9939 - recall: 0.9259 - val_accuracy: 0.9884 - val_loss: 0.0412 - val_precision: 0.9957 - val_recall: 0.9263 - learning_rate: 5.0000e-04\nEpoch 17/20\n\u001b[1m9369/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9882 - loss: 0.0431 - precision: 0.9943 - recall: 0.9260\nEpoch 17: val_accuracy improved from 0.98838 to 0.98838, saving model to /kaggle/working/cnn_fold2_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9882 - loss: 0.0431 - precision: 0.9943 - recall: 0.9260 - val_accuracy: 0.9884 - val_loss: 0.0409 - val_precision: 0.9961 - val_recall: 0.9259 - learning_rate: 5.0000e-04\nEpoch 18/20\n\u001b[1m9370/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9881 - loss: 0.0431 - precision: 0.9942 - recall: 0.9261\nEpoch 18: val_accuracy improved from 0.98838 to 0.98839, saving model to /kaggle/working/cnn_fold2_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9881 - loss: 0.0431 - precision: 0.9942 - recall: 0.9261 - val_accuracy: 0.9884 - val_loss: 0.0410 - val_precision: 0.9947 - val_recall: 0.9273 - learning_rate: 5.0000e-04\nEpoch 19/20\n\u001b[1m9368/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0432 - precision: 0.9939 - recall: 0.9257\nEpoch 19: val_accuracy did not improve from 0.98839\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9880 - loss: 0.0432 - precision: 0.9939 - recall: 0.9257 - val_accuracy: 0.9883 - val_loss: 0.0410 - val_precision: 0.9915 - val_recall: 0.9296 - learning_rate: 5.0000e-04\nEpoch 20/20\n\u001b[1m9367/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0432 - precision: 0.9939 - recall: 0.9255\nEpoch 20: val_accuracy did not improve from 0.98839\n\nEpoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9880 - loss: 0.0432 - precision: 0.9939 - recall: 0.9255 - val_accuracy: 0.9883 - val_loss: 0.0413 - val_precision: 0.9920 - val_recall: 0.9292 - learning_rate: 5.0000e-04\nRestoring model weights from the end of the best epoch: 17.\n\nFOLD 2 RESULTS:\n  Accuracy:  0.9884\n  Precision: 0.9961\n  Recall:    0.9259\n  F1-score:  0.9597\n\n======================================================================\nTRAINING CNN - FOLD 3/5\n======================================================================\nX_train: (2399995, 77, 1), y_train: (2399995,)\nX_test:  (599999, 77, 1), y_test:  (599999,)\nEpoch 1/20\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9679 - loss: 0.0990 - precision: 0.9312 - recall: 0.8487\nEpoch 1: val_accuracy improved from -inf to 0.98725, saving model to /kaggle/working/cnn_fold3_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 7ms/step - accuracy: 0.9679 - loss: 0.0990 - precision: 0.9312 - recall: 0.8487 - val_accuracy: 0.9873 - val_loss: 0.0514 - val_precision: 0.9936 - val_recall: 0.9206 - learning_rate: 0.0010\nEpoch 2/20\n\u001b[1m9366/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9869 - loss: 0.0536 - precision: 0.9908 - recall: 0.9207\nEpoch 2: val_accuracy improved from 0.98725 to 0.98763, saving model to /kaggle/working/cnn_fold3_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9869 - loss: 0.0536 - precision: 0.9908 - recall: 0.9207 - val_accuracy: 0.9876 - val_loss: 0.0479 - val_precision: 0.9931 - val_recall: 0.9236 - learning_rate: 0.0010\nEpoch 3/20\n\u001b[1m9373/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9875 - loss: 0.0502 - precision: 0.9921 - recall: 0.9235\nEpoch 3: val_accuracy did not improve from 0.98763\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - accuracy: 0.9875 - loss: 0.0502 - precision: 0.9921 - recall: 0.9235 - val_accuracy: 0.9874 - val_loss: 0.0486 - val_precision: 0.9920 - val_recall: 0.9230 - learning_rate: 0.0010\nEpoch 4/20\n\u001b[1m9370/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9876 - loss: 0.0489 - precision: 0.9927 - recall: 0.9237\nEpoch 4: val_accuracy improved from 0.98763 to 0.98770, saving model to /kaggle/working/cnn_fold3_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9876 - loss: 0.0489 - precision: 0.9927 - recall: 0.9237 - val_accuracy: 0.9877 - val_loss: 0.0460 - val_precision: 0.9937 - val_recall: 0.9236 - learning_rate: 0.0010\nEpoch 5/20\n\u001b[1m9374/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9878 - loss: 0.0479 - precision: 0.9933 - recall: 0.9244\nEpoch 5: val_accuracy improved from 0.98770 to 0.98777, saving model to /kaggle/working/cnn_fold3_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9878 - loss: 0.0479 - precision: 0.9933 - recall: 0.9244 - val_accuracy: 0.9878 - val_loss: 0.0460 - val_precision: 0.9936 - val_recall: 0.9241 - learning_rate: 0.0010\nEpoch 6/20\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9878 - loss: 0.0472 - precision: 0.9938 - recall: 0.9244\nEpoch 6: val_accuracy improved from 0.98777 to 0.98785, saving model to /kaggle/working/cnn_fold3_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - accuracy: 0.9878 - loss: 0.0472 - precision: 0.9938 - recall: 0.9244 - val_accuracy: 0.9878 - val_loss: 0.0456 - val_precision: 0.9937 - val_recall: 0.9245 - learning_rate: 0.0010\nEpoch 7/20\n\u001b[1m9366/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0466 - precision: 0.9935 - recall: 0.9250\nEpoch 7: val_accuracy improved from 0.98785 to 0.98798, saving model to /kaggle/working/cnn_fold3_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0466 - precision: 0.9935 - recall: 0.9250 - val_accuracy: 0.9880 - val_loss: 0.0450 - val_precision: 0.9951 - val_recall: 0.9241 - learning_rate: 0.0010\nEpoch 8/20\n\u001b[1m9372/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0464 - precision: 0.9934 - recall: 0.9254\nEpoch 8: val_accuracy improved from 0.98798 to 0.98799, saving model to /kaggle/working/cnn_fold3_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0464 - precision: 0.9934 - recall: 0.9254 - val_accuracy: 0.9880 - val_loss: 0.0446 - val_precision: 0.9959 - val_recall: 0.9234 - learning_rate: 0.0010\nEpoch 9/20\n\u001b[1m9371/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0463 - precision: 0.9933 - recall: 0.9250\nEpoch 9: val_accuracy improved from 0.98799 to 0.98802, saving model to /kaggle/working/cnn_fold3_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0463 - precision: 0.9933 - recall: 0.9250 - val_accuracy: 0.9880 - val_loss: 0.0446 - val_precision: 0.9947 - val_recall: 0.9247 - learning_rate: 0.0010\nEpoch 10/20\n\u001b[1m9366/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9878 - loss: 0.0460 - precision: 0.9934 - recall: 0.9246\nEpoch 10: val_accuracy did not improve from 0.98802\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9878 - loss: 0.0460 - precision: 0.9934 - recall: 0.9246 - val_accuracy: 0.9879 - val_loss: 0.0437 - val_precision: 0.9948 - val_recall: 0.9241 - learning_rate: 0.0010\nEpoch 11/20\n\u001b[1m9367/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9878 - loss: 0.0462 - precision: 0.9934 - recall: 0.9245\nEpoch 11: val_accuracy improved from 0.98802 to 0.98807, saving model to /kaggle/working/cnn_fold3_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9878 - loss: 0.0462 - precision: 0.9934 - recall: 0.9245 - val_accuracy: 0.9881 - val_loss: 0.0435 - val_precision: 0.9956 - val_recall: 0.9243 - learning_rate: 0.0010\nEpoch 12/20\n\u001b[1m9371/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0454 - precision: 0.9937 - recall: 0.9255\nEpoch 12: val_accuracy did not improve from 0.98807\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - accuracy: 0.9880 - loss: 0.0454 - precision: 0.9937 - recall: 0.9255 - val_accuracy: 0.9879 - val_loss: 0.0441 - val_precision: 0.9944 - val_recall: 0.9244 - learning_rate: 0.0010\nEpoch 13/20\n\u001b[1m9374/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0452 - precision: 0.9940 - recall: 0.9254\nEpoch 13: val_accuracy improved from 0.98807 to 0.98809, saving model to /kaggle/working/cnn_fold3_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9880 - loss: 0.0452 - precision: 0.9940 - recall: 0.9254 - val_accuracy: 0.9881 - val_loss: 0.0434 - val_precision: 0.9953 - val_recall: 0.9247 - learning_rate: 0.0010\nEpoch 14/20\n\u001b[1m9366/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0451 - precision: 0.9937 - recall: 0.9251\nEpoch 14: val_accuracy did not improve from 0.98809\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0451 - precision: 0.9937 - recall: 0.9251 - val_accuracy: 0.9879 - val_loss: 0.0431 - val_precision: 0.9900 - val_recall: 0.9283 - learning_rate: 0.0010\nEpoch 15/20\n\u001b[1m9367/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9881 - loss: 0.0444 - precision: 0.9933 - recall: 0.9266\nEpoch 15: val_accuracy did not improve from 0.98809\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9881 - loss: 0.0444 - precision: 0.9933 - recall: 0.9266 - val_accuracy: 0.9881 - val_loss: 0.0428 - val_precision: 0.9957 - val_recall: 0.9243 - learning_rate: 0.0010\nEpoch 16/20\n\u001b[1m9366/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9881 - loss: 0.0447 - precision: 0.9940 - recall: 0.9261\nEpoch 16: val_accuracy did not improve from 0.98809\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - accuracy: 0.9881 - loss: 0.0447 - precision: 0.9940 - recall: 0.9261 - val_accuracy: 0.9879 - val_loss: 0.0436 - val_precision: 0.9949 - val_recall: 0.9236 - learning_rate: 0.0010\nEpoch 17/20\n\u001b[1m9369/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9881 - loss: 0.0443 - precision: 0.9935 - recall: 0.9266\nEpoch 17: val_accuracy did not improve from 0.98809\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9881 - loss: 0.0443 - precision: 0.9935 - recall: 0.9266 - val_accuracy: 0.9876 - val_loss: 0.0430 - val_precision: 0.9846 - val_recall: 0.9319 - learning_rate: 0.0010\nEpoch 18/20\n\u001b[1m9366/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0444 - precision: 0.9939 - recall: 0.9252\nEpoch 18: val_accuracy did not improve from 0.98809\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - accuracy: 0.9880 - loss: 0.0444 - precision: 0.9939 - recall: 0.9252 - val_accuracy: 0.9879 - val_loss: 0.0425 - val_precision: 0.9875 - val_recall: 0.9310 - learning_rate: 0.0010\nEpoch 19/20\n\u001b[1m9367/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0442 - precision: 0.9933 - recall: 0.9260\nEpoch 19: val_accuracy improved from 0.98809 to 0.98810, saving model to /kaggle/working/cnn_fold3_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 6ms/step - accuracy: 0.9880 - loss: 0.0442 - precision: 0.9933 - recall: 0.9260 - val_accuracy: 0.9881 - val_loss: 0.0422 - val_precision: 0.9955 - val_recall: 0.9246 - learning_rate: 0.0010\nEpoch 20/20\n\u001b[1m9373/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9881 - loss: 0.0441 - precision: 0.9939 - recall: 0.9257\nEpoch 20: val_accuracy improved from 0.98810 to 0.98813, saving model to /kaggle/working/cnn_fold3_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9881 - loss: 0.0441 - precision: 0.9939 - recall: 0.9257 - val_accuracy: 0.9881 - val_loss: 0.0419 - val_precision: 0.9956 - val_recall: 0.9247 - learning_rate: 0.0010\nRestoring model weights from the end of the best epoch: 20.\n\nFOLD 3 RESULTS:\n  Accuracy:  0.9881\n  Precision: 0.9956\n  Recall:    0.9247\n  F1-score:  0.9588\n\n======================================================================\nTRAINING CNN - FOLD 4/5\n======================================================================\nX_train: (2399995, 77, 1), y_train: (2399995,)\nX_test:  (599999, 77, 1), y_test:  (599999,)\nEpoch 1/20\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9687 - loss: 0.0973 - precision: 0.9359 - recall: 0.8471\nEpoch 1: val_accuracy improved from -inf to 0.98714, saving model to /kaggle/working/cnn_fold4_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 7ms/step - accuracy: 0.9687 - loss: 0.0973 - precision: 0.9359 - recall: 0.8471 - val_accuracy: 0.9871 - val_loss: 0.0518 - val_precision: 0.9906 - val_recall: 0.9227 - learning_rate: 0.0010\nEpoch 2/20\n\u001b[1m9371/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9870 - loss: 0.0531 - precision: 0.9918 - recall: 0.9210\nEpoch 2: val_accuracy improved from 0.98714 to 0.98731, saving model to /kaggle/working/cnn_fold4_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9870 - loss: 0.0531 - precision: 0.9918 - recall: 0.9210 - val_accuracy: 0.9873 - val_loss: 0.0487 - val_precision: 0.9841 - val_recall: 0.9302 - learning_rate: 0.0010\nEpoch 3/20\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9875 - loss: 0.0500 - precision: 0.9925 - recall: 0.9237\nEpoch 3: val_accuracy improved from 0.98731 to 0.98776, saving model to /kaggle/working/cnn_fold4_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9875 - loss: 0.0500 - precision: 0.9925 - recall: 0.9237 - val_accuracy: 0.9878 - val_loss: 0.0467 - val_precision: 0.9938 - val_recall: 0.9239 - learning_rate: 0.0010\nEpoch 4/20\n\u001b[1m9371/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9876 - loss: 0.0489 - precision: 0.9926 - recall: 0.9240\nEpoch 4: val_accuracy did not improve from 0.98776\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9876 - loss: 0.0489 - precision: 0.9926 - recall: 0.9240 - val_accuracy: 0.9877 - val_loss: 0.0468 - val_precision: 0.9928 - val_recall: 0.9245 - learning_rate: 0.0010\nEpoch 5/20\n\u001b[1m9369/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9878 - loss: 0.0478 - precision: 0.9935 - recall: 0.9246\nEpoch 5: val_accuracy did not improve from 0.98776\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9878 - loss: 0.0478 - precision: 0.9935 - recall: 0.9246 - val_accuracy: 0.9876 - val_loss: 0.0454 - val_precision: 0.9846 - val_recall: 0.9315 - learning_rate: 0.0010\nEpoch 6/20\n\u001b[1m9372/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0470 - precision: 0.9933 - recall: 0.9251\nEpoch 6: val_accuracy improved from 0.98776 to 0.98795, saving model to /kaggle/working/cnn_fold4_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0470 - precision: 0.9933 - recall: 0.9251 - val_accuracy: 0.9879 - val_loss: 0.0448 - val_precision: 0.9942 - val_recall: 0.9248 - learning_rate: 0.0010\nEpoch 7/20\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0466 - precision: 0.9939 - recall: 0.9245\nEpoch 7: val_accuracy improved from 0.98795 to 0.98803, saving model to /kaggle/working/cnn_fold4_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0466 - precision: 0.9939 - recall: 0.9245 - val_accuracy: 0.9880 - val_loss: 0.0443 - val_precision: 0.9945 - val_recall: 0.9251 - learning_rate: 0.0010\nEpoch 8/20\n\u001b[1m9372/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9878 - loss: 0.0463 - precision: 0.9936 - recall: 0.9246\nEpoch 8: val_accuracy did not improve from 0.98803\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9878 - loss: 0.0463 - precision: 0.9936 - recall: 0.9246 - val_accuracy: 0.9880 - val_loss: 0.0438 - val_precision: 0.9918 - val_recall: 0.9274 - learning_rate: 0.0010\nEpoch 9/20\n\u001b[1m9369/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0460 - precision: 0.9936 - recall: 0.9253\nEpoch 9: val_accuracy did not improve from 0.98803\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0460 - precision: 0.9936 - recall: 0.9253 - val_accuracy: 0.9879 - val_loss: 0.0445 - val_precision: 0.9944 - val_recall: 0.9245 - learning_rate: 0.0010\nEpoch 10/20\n\u001b[1m9367/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0454 - precision: 0.9936 - recall: 0.9253\nEpoch 10: val_accuracy did not improve from 0.98803\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9880 - loss: 0.0454 - precision: 0.9936 - recall: 0.9253 - val_accuracy: 0.9879 - val_loss: 0.0439 - val_precision: 0.9941 - val_recall: 0.9245 - learning_rate: 0.0010\nEpoch 11/20\n\u001b[1m9367/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0452 - precision: 0.9938 - recall: 0.9253\nEpoch 11: val_accuracy did not improve from 0.98803\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9880 - loss: 0.0452 - precision: 0.9938 - recall: 0.9253 - val_accuracy: 0.9878 - val_loss: 0.0431 - val_precision: 0.9861 - val_recall: 0.9317 - learning_rate: 0.0010\nEpoch 12/20\n\u001b[1m9373/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0450 - precision: 0.9934 - recall: 0.9254\nEpoch 12: val_accuracy improved from 0.98803 to 0.98805, saving model to /kaggle/working/cnn_fold4_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0450 - precision: 0.9934 - recall: 0.9254 - val_accuracy: 0.9880 - val_loss: 0.0433 - val_precision: 0.9948 - val_recall: 0.9248 - learning_rate: 0.0010\nEpoch 13/20\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0447 - precision: 0.9939 - recall: 0.9252\nEpoch 13: val_accuracy improved from 0.98805 to 0.98811, saving model to /kaggle/working/cnn_fold4_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9880 - loss: 0.0447 - precision: 0.9939 - recall: 0.9252 - val_accuracy: 0.9881 - val_loss: 0.0425 - val_precision: 0.9954 - val_recall: 0.9247 - learning_rate: 0.0010\nEpoch 14/20\n\u001b[1m9374/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0447 - precision: 0.9939 - recall: 0.9248\nEpoch 14: val_accuracy did not improve from 0.98811\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0447 - precision: 0.9939 - recall: 0.9248 - val_accuracy: 0.9880 - val_loss: 0.0430 - val_precision: 0.9896 - val_recall: 0.9297 - learning_rate: 0.0010\nEpoch 15/20\n\u001b[1m9372/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0442 - precision: 0.9935 - recall: 0.9259\nEpoch 15: val_accuracy did not improve from 0.98811\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9880 - loss: 0.0442 - precision: 0.9935 - recall: 0.9259 - val_accuracy: 0.9879 - val_loss: 0.0426 - val_precision: 0.9884 - val_recall: 0.9303 - learning_rate: 0.0010\nEpoch 16/20\n\u001b[1m9370/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0445 - precision: 0.9937 - recall: 0.9248\nEpoch 16: val_accuracy improved from 0.98811 to 0.98820, saving model to /kaggle/working/cnn_fold4_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0445 - precision: 0.9937 - recall: 0.9248 - val_accuracy: 0.9882 - val_loss: 0.0421 - val_precision: 0.9955 - val_recall: 0.9252 - learning_rate: 0.0010\nEpoch 17/20\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9881 - loss: 0.0436 - precision: 0.9939 - recall: 0.9264\nEpoch 17: val_accuracy improved from 0.98820 to 0.98826, saving model to /kaggle/working/cnn_fold4_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9881 - loss: 0.0436 - precision: 0.9939 - recall: 0.9264 - val_accuracy: 0.9883 - val_loss: 0.0418 - val_precision: 0.9961 - val_recall: 0.9250 - learning_rate: 0.0010\nEpoch 18/20\n\u001b[1m9370/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9881 - loss: 0.0440 - precision: 0.9940 - recall: 0.9256\nEpoch 18: val_accuracy did not improve from 0.98826\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9881 - loss: 0.0440 - precision: 0.9940 - recall: 0.9256 - val_accuracy: 0.9878 - val_loss: 0.0426 - val_precision: 0.9857 - val_recall: 0.9319 - learning_rate: 0.0010\nEpoch 19/20\n\u001b[1m9367/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9881 - loss: 0.0439 - precision: 0.9937 - recall: 0.9258\nEpoch 19: val_accuracy did not improve from 0.98826\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9881 - loss: 0.0439 - precision: 0.9937 - recall: 0.9258 - val_accuracy: 0.9879 - val_loss: 0.0422 - val_precision: 0.9870 - val_recall: 0.9313 - learning_rate: 0.0010\nEpoch 20/20\n\u001b[1m9372/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9881 - loss: 0.0439 - precision: 0.9940 - recall: 0.9256\nEpoch 20: val_accuracy did not improve from 0.98826\n\nEpoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9881 - loss: 0.0439 - precision: 0.9940 - recall: 0.9256 - val_accuracy: 0.9882 - val_loss: 0.0418 - val_precision: 0.9928 - val_recall: 0.9275 - learning_rate: 0.0010\nRestoring model weights from the end of the best epoch: 17.\n\nFOLD 4 RESULTS:\n  Accuracy:  0.9883\n  Precision: 0.9961\n  Recall:    0.9250\n  F1-score:  0.9593\n\n======================================================================\nTRAINING CNN - FOLD 5/5\n======================================================================\nX_train: (2399996, 77, 1), y_train: (2399996,)\nX_test:  (599998, 77, 1), y_test:  (599998,)\nEpoch 1/20\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9691 - loss: 0.0972 - precision: 0.9363 - recall: 0.8493\nEpoch 1: val_accuracy improved from -inf to 0.98759, saving model to /kaggle/working/cnn_fold5_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 7ms/step - accuracy: 0.9691 - loss: 0.0972 - precision: 0.9363 - recall: 0.8493 - val_accuracy: 0.9876 - val_loss: 0.0502 - val_precision: 0.9927 - val_recall: 0.9237 - learning_rate: 0.0010\nEpoch 2/20\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9871 - loss: 0.0525 - precision: 0.9910 - recall: 0.9218\nEpoch 2: val_accuracy improved from 0.98759 to 0.98774, saving model to /kaggle/working/cnn_fold5_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9871 - loss: 0.0525 - precision: 0.9910 - recall: 0.9218 - val_accuracy: 0.9877 - val_loss: 0.0479 - val_precision: 0.9923 - val_recall: 0.9251 - learning_rate: 0.0010\nEpoch 3/20\n\u001b[1m9369/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9874 - loss: 0.0503 - precision: 0.9925 - recall: 0.9226\nEpoch 3: val_accuracy did not improve from 0.98774\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9874 - loss: 0.0503 - precision: 0.9925 - recall: 0.9226 - val_accuracy: 0.9877 - val_loss: 0.0468 - val_precision: 0.9835 - val_recall: 0.9331 - learning_rate: 0.0010\nEpoch 4/20\n\u001b[1m9373/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9874 - loss: 0.0491 - precision: 0.9926 - recall: 0.9227\nEpoch 4: val_accuracy improved from 0.98774 to 0.98807, saving model to /kaggle/working/cnn_fold5_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9874 - loss: 0.0491 - precision: 0.9926 - recall: 0.9227 - val_accuracy: 0.9881 - val_loss: 0.0448 - val_precision: 0.9945 - val_recall: 0.9253 - learning_rate: 0.0010\nEpoch 5/20\n\u001b[1m9370/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9877 - loss: 0.0476 - precision: 0.9927 - recall: 0.9244\nEpoch 5: val_accuracy improved from 0.98807 to 0.98808, saving model to /kaggle/working/cnn_fold5_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9877 - loss: 0.0476 - precision: 0.9927 - recall: 0.9244 - val_accuracy: 0.9881 - val_loss: 0.0451 - val_precision: 0.9944 - val_recall: 0.9254 - learning_rate: 0.0010\nEpoch 6/20\n\u001b[1m9367/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9876 - loss: 0.0477 - precision: 0.9930 - recall: 0.9232\nEpoch 6: val_accuracy improved from 0.98808 to 0.98819, saving model to /kaggle/working/cnn_fold5_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9876 - loss: 0.0477 - precision: 0.9930 - recall: 0.9232 - val_accuracy: 0.9882 - val_loss: 0.0443 - val_precision: 0.9943 - val_recall: 0.9263 - learning_rate: 0.0010\nEpoch 7/20\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9878 - loss: 0.0466 - precision: 0.9932 - recall: 0.9246\nEpoch 7: val_accuracy did not improve from 0.98819\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9878 - loss: 0.0466 - precision: 0.9932 - recall: 0.9246 - val_accuracy: 0.9877 - val_loss: 0.0467 - val_precision: 0.9928 - val_recall: 0.9246 - learning_rate: 0.0010\nEpoch 8/20\n\u001b[1m9369/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9877 - loss: 0.0467 - precision: 0.9931 - recall: 0.9243\nEpoch 8: val_accuracy did not improve from 0.98819\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9877 - loss: 0.0467 - precision: 0.9931 - recall: 0.9243 - val_accuracy: 0.9880 - val_loss: 0.0438 - val_precision: 0.9860 - val_recall: 0.9328 - learning_rate: 0.0010\nEpoch 9/20\n\u001b[1m9369/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9878 - loss: 0.0461 - precision: 0.9936 - recall: 0.9245\nEpoch 9: val_accuracy did not improve from 0.98819\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9878 - loss: 0.0461 - precision: 0.9936 - recall: 0.9245 - val_accuracy: 0.9880 - val_loss: 0.0434 - val_precision: 0.9868 - val_recall: 0.9322 - learning_rate: 0.0010\nEpoch 10/20\n\u001b[1m9366/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0457 - precision: 0.9936 - recall: 0.9250\nEpoch 10: val_accuracy improved from 0.98819 to 0.98827, saving model to /kaggle/working/cnn_fold5_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0457 - precision: 0.9936 - recall: 0.9250 - val_accuracy: 0.9883 - val_loss: 0.0431 - val_precision: 0.9952 - val_recall: 0.9260 - learning_rate: 0.0010\nEpoch 11/20\n\u001b[1m9366/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0457 - precision: 0.9937 - recall: 0.9246\nEpoch 11: val_accuracy did not improve from 0.98827\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0457 - precision: 0.9937 - recall: 0.9246 - val_accuracy: 0.9882 - val_loss: 0.0432 - val_precision: 0.9942 - val_recall: 0.9266 - learning_rate: 0.0010\nEpoch 12/20\n\u001b[1m9372/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0455 - precision: 0.9934 - recall: 0.9253\nEpoch 12: val_accuracy did not improve from 0.98827\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0455 - precision: 0.9934 - recall: 0.9253 - val_accuracy: 0.9883 - val_loss: 0.0432 - val_precision: 0.9949 - val_recall: 0.9262 - learning_rate: 0.0010\nEpoch 13/20\n\u001b[1m9373/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0453 - precision: 0.9937 - recall: 0.9250\nEpoch 13: val_accuracy improved from 0.98827 to 0.98835, saving model to /kaggle/working/cnn_fold5_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0453 - precision: 0.9937 - recall: 0.9250 - val_accuracy: 0.9884 - val_loss: 0.0428 - val_precision: 0.9952 - val_recall: 0.9265 - learning_rate: 0.0010\nEpoch 14/20\n\u001b[1m9366/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0446 - precision: 0.9935 - recall: 0.9258\nEpoch 14: val_accuracy did not improve from 0.98835\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9880 - loss: 0.0446 - precision: 0.9935 - recall: 0.9258 - val_accuracy: 0.9883 - val_loss: 0.0428 - val_precision: 0.9961 - val_recall: 0.9251 - learning_rate: 0.0010\nEpoch 15/20\n\u001b[1m9366/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0449 - precision: 0.9934 - recall: 0.9251\nEpoch 15: val_accuracy did not improve from 0.98835\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9879 - loss: 0.0449 - precision: 0.9934 - recall: 0.9251 - val_accuracy: 0.9883 - val_loss: 0.0429 - val_precision: 0.9946 - val_recall: 0.9270 - learning_rate: 0.0010\nEpoch 16/20\n\u001b[1m9371/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0446 - precision: 0.9936 - recall: 0.9257\nEpoch 16: val_accuracy did not improve from 0.98835\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9880 - loss: 0.0446 - precision: 0.9936 - recall: 0.9257 - val_accuracy: 0.9878 - val_loss: 0.0427 - val_precision: 0.9829 - val_recall: 0.9347 - learning_rate: 0.0010\nEpoch 17/20\n\u001b[1m9374/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0443 - precision: 0.9934 - recall: 0.9258\nEpoch 17: val_accuracy did not improve from 0.98835\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9880 - loss: 0.0443 - precision: 0.9934 - recall: 0.9258 - val_accuracy: 0.9883 - val_loss: 0.0423 - val_precision: 0.9947 - val_recall: 0.9265 - learning_rate: 0.0010\nEpoch 18/20\n\u001b[1m9367/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0440 - precision: 0.9935 - recall: 0.9256\nEpoch 18: val_accuracy did not improve from 0.98835\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9880 - loss: 0.0440 - precision: 0.9935 - recall: 0.9256 - val_accuracy: 0.9883 - val_loss: 0.0416 - val_precision: 0.9952 - val_recall: 0.9264 - learning_rate: 0.0010\nEpoch 19/20\n\u001b[1m9373/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9878 - loss: 0.0444 - precision: 0.9932 - recall: 0.9246\nEpoch 19: val_accuracy improved from 0.98835 to 0.98840, saving model to /kaggle/working/cnn_fold5_best.keras\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9878 - loss: 0.0444 - precision: 0.9932 - recall: 0.9246 - val_accuracy: 0.9884 - val_loss: 0.0415 - val_precision: 0.9959 - val_recall: 0.9263 - learning_rate: 0.0010\nEpoch 20/20\n\u001b[1m9372/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9880 - loss: 0.0440 - precision: 0.9936 - recall: 0.9255\nEpoch 20: val_accuracy did not improve from 0.98840\n\u001b[1m9375/9375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6ms/step - accuracy: 0.9880 - loss: 0.0440 - precision: 0.9936 - recall: 0.9255 - val_accuracy: 0.9881 - val_loss: 0.0424 - val_precision: 0.9865 - val_recall: 0.9329 - learning_rate: 0.0010\nRestoring model weights from the end of the best epoch: 19.\n\nFOLD 5 RESULTS:\n  Accuracy:  0.9884\n  Precision: 0.9959\n  Recall:    0.9263\n  F1-score:  0.9598\n\n======================================================================\nCNN K-FOLD SUMMARY\n======================================================================\n   fold  accuracy  precision    recall        f1\n0     1  0.988432   0.989990  0.932016  0.960129\n1     2  0.988385   0.996148  0.925860  0.959719\n2     3  0.988133   0.995593  0.924690  0.958832\n3     4  0.988257   0.996097  0.925047  0.959258\n4     5  0.988402   0.995851  0.926250  0.959791\n\nMEAN METRICS:\naccuracy     0.988322\nprecision    0.994736\nrecall       0.926772\nf1           0.959546\ndtype: float64\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Kiá»ƒm tra GPU\nprint(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\nprint(f\"Mixed precision: {tf.keras.mixed_precision.global_policy()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T08:39:34.854711Z","iopub.execute_input":"2025-12-05T08:39:34.855413Z","iopub.status.idle":"2025-12-05T08:39:34.859818Z","shell.execute_reply.started":"2025-12-05T08:39:34.855388Z","shell.execute_reply":"2025-12-05T08:39:34.858959Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\nMixed precision: <DTypePolicy \"float32\">\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# train_lstm.py\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom pathlib import Path\nimport pandas as pd\nimport gc\n\n# Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n\nBASE_FOLD_DIR = Path('/kaggle/input/csecicids2018-splited/folds')\nN_FOLDS = 5\n\nprint(\"=\"*70)\nprint(\"LSTM TRAINING FOR CICIDS2018 - BINARY CLASSIFICATION\")\nprint(\"=\"*70)\n\ndef build_lstm(input_shape):\n    \"\"\"\n    XÃ¢y dá»±ng mÃ´ hÃ¬nh LSTM tá»‘i Æ°u tá»‘c Ä‘á»™ cho phÃ¡t hiá»‡n traffic báº¥t thÆ°á»ng\n    \"\"\"\n    model = keras.Sequential([\n        layers.LSTM(64, return_sequences=True, input_shape=input_shape),\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        \n        # LSTM Layer 2 \n        layers.LSTM(32, return_sequences=False),\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        \n        # Dense Layers \n        layers.Dense(64, activation='relu'),\n        layers.Dropout(0.4),\n        layers.Dense(1, activation='sigmoid')  # Binary classification\n    ])\n    \n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n        loss='binary_crossentropy',\n        metrics=[\n            'accuracy',\n            tf.keras.metrics.Precision(name='precision'),\n            tf.keras.metrics.Recall(name='recall'),\n            tf.keras.metrics.AUC(name='auc')\n        ]\n    )\n    return model\n\n# LÆ°u káº¿t quáº£ tá»«ng fold\nfold_results = []\n\nfor fold in range(1, N_FOLDS + 1):\n    print(\"\\n\" + \"=\"*70)\n    print(f\"TRAINING LSTM - FOLD {fold}/{N_FOLDS}\")\n    print(\"=\"*70)\n    \n    # Load dá»¯ liá»‡u Ä‘Ã£ split\n    fold_dir = BASE_FOLD_DIR / f'fold_{fold}'\n    \n    print(f\"\\nLoading data from: {fold_dir}\")\n    X_train = np.load(fold_dir / 'X_train.npy')\n    y_train = np.load(fold_dir / 'y_train.npy')\n    X_test  = np.load(fold_dir / 'X_test.npy')\n    y_test  = np.load(fold_dir / 'y_test.npy')\n    \n    # Reshape cho LSTM: (samples, timesteps, features)\n    # LSTM cáº§n dáº¡ng 3D: (batch_size, timesteps, features)\n    # á» Ä‘Ã¢y ta coi má»—i feature nhÆ° 1 timestep\n    X_train_lstm = X_train.reshape(-1, X_train.shape[1], 1)\n    X_test_lstm  = X_test.reshape(-1, X_test.shape[1], 1)\n    \n    print(f\"\\nData shapes:\")\n    print(f\"  X_train: {X_train_lstm.shape}, y_train: {y_train.shape}\")\n    print(f\"  X_test:  {X_test_lstm.shape}, y_test:  {y_test.shape}\")\n    \n    print(f\"\\nClass distribution:\")\n    print(f\"  Train - Benign: {(y_train==0).sum():,} ({(y_train==0).sum()/len(y_train)*100:.2f}%)\")\n    print(f\"  Train - Attack: {(y_train==1).sum():,} ({(y_train==1).sum()/len(y_train)*100:.2f}%)\")\n    print(f\"  Test  - Benign: {(y_test==0).sum():,} ({(y_test==0).sum()/len(y_test)*100:.2f}%)\")\n    print(f\"  Test  - Attack: {(y_test==1).sum():,} ({(y_test==1).sum()/len(y_test)*100:.2f}%)\")\n    \n    # Build model\n    print(\"\\nBuilding LSTM model...\")\n    model = build_lstm(input_shape=(X_train_lstm.shape[1], 1))\n    \n    print(\"\\nModel Summary:\")\n    model.summary()\n    \n    # Callbacks\n    checkpoint_path = f'/kaggle/working/lstm_fold{fold}_best.keras'\n    callbacks = [\n        keras.callbacks.ModelCheckpoint(\n            checkpoint_path, \n            monitor='val_auc',\n            save_best_only=True, \n            mode='max', \n            verbose=1\n        ),\n        keras.callbacks.EarlyStopping(\n            monitor='val_loss', \n            patience=5,\n            restore_best_weights=True, \n            verbose=1\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss', \n            factor=0.5,\n            patience=3, \n            min_lr=1e-6, \n            verbose=1\n        )\n    ]\n    \n    # Training\n    print(f\"\\nStarting training for Fold {fold}...\")\n    history = model.fit(\n        X_train_lstm, y_train,\n        epochs=20,  \n        batch_size=512,  \n        validation_data=(X_test_lstm, y_test),\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    # Evaluate\n    print(f\"\\nEvaluating Fold {fold}...\")\n    test_results = model.evaluate(X_test_lstm, y_test, verbose=0)\n    loss = test_results[0]\n    acc = test_results[1]\n    prec = test_results[2]\n    rec = test_results[3]\n    auc = test_results[4]\n    \n    # Calculate F1-score\n    f1 = 2 * prec * rec / (prec + rec + 1e-8)\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"FOLD {fold} RESULTS:\")\n    print(f\"{'='*70}\")\n    print(f\"  Loss:      {loss:.4f}\")\n    print(f\"  Accuracy:  {acc:.4f}\")\n    print(f\"  Precision: {prec:.4f}\")\n    print(f\"  Recall:    {rec:.4f}\")\n    print(f\"  F1-score:  {f1:.4f}\")\n    print(f\"  AUC:       {auc:.4f}\")\n    print(f\"{'='*70}\")\n    \n    fold_results.append({\n        'fold': fold,\n        'loss': loss,\n        'accuracy': acc,\n        'precision': prec,\n        'recall': rec,\n        'f1': f1,\n        'auc': auc\n    })\n    \n    # Clean up memory\n    del X_train, y_train, X_test, y_test\n    del X_train_lstm, X_test_lstm\n    del model\n    gc.collect()\n    tf.keras.backend.clear_session()\n\n# Tá»•ng há»£p káº¿t quáº£ táº¥t cáº£ cÃ¡c fold\nprint(\"\\n\" + \"=\"*70)\nprint(\"LSTM K-FOLD CROSS-VALIDATION SUMMARY\")\nprint(\"=\"*70)\n\nresults_df = pd.DataFrame(fold_results)\nprint(\"\\nResults per fold:\")\nprint(results_df.to_string(index=False))\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"MEAN METRICS ACROSS ALL FOLDS:\")\nprint(\"=\"*70)\nmean_metrics = results_df[['loss', 'accuracy', 'precision', 'recall', 'f1', 'auc']].mean()\nstd_metrics = results_df[['loss', 'accuracy', 'precision', 'recall', 'f1', 'auc']].std()\n\nfor metric in ['loss', 'accuracy', 'precision', 'recall', 'f1', 'auc']:\n    print(f\"  {metric.capitalize():12s}: {mean_metrics[metric]:.4f} Â± {std_metrics[metric]:.4f}\")\n\nprint(\"=\"*70)\nprint(\"âœ… LSTM TRAINING COMPLETED!\")\nprint(\"=\"*70)\n\n# Save results to CSV\nresults_df.to_csv('/kaggle/working/lstm_kfold_results.csv', index=False)\nprint(\"\\nğŸ“Š Results saved to: /kaggle/working/lstm_kfold_results.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T06:00:22.553719Z","iopub.execute_input":"2025-12-05T06:00:22.554312Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"======================================================================\nLSTM TRAINING FOR CICIDS2018 - BINARY CLASSIFICATION\n======================================================================\n\n======================================================================\nTRAINING LSTM - FOLD 1/5\n======================================================================\n\nLoading data from: /kaggle/input/csecicids2018-splited/folds/fold_1\n\nData shapes:\n  X_train: (2399995, 77, 1), y_train: (2399995,)\n  X_test:  (599999, 77, 1), y_test:  (599999,)\n\nClass distribution:\n  Train - Benign: 2,041,321 (85.06%)\n  Train - Attack: 358,674 (14.94%)\n  Test  - Benign: 510,331 (85.06%)\n  Test  - Attack: 89,668 (14.94%)\n\nBuilding LSTM model...\n\nModel Summary:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m77\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚        \u001b[38;5;34m16,896\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_3           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m77\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚           \u001b[38;5;34m256\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m77\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚        \u001b[38;5;34m12,416\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_4           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚           \u001b[38;5;34m128\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m2,112\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m65\u001b[0m â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">77</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_3           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">77</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">77</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_4           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m31,873\u001b[0m (124.50 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,873</span> (124.50 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m31,681\u001b[0m (123.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,681</span> (123.75 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\nStarting training for Fold 1...\nEpoch 1/20\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9431 - auc: 0.9398 - loss: 0.1653 - precision: 0.8724 - recall: 0.7133\nEpoch 1: val_auc improved from -inf to 0.97610, saving model to /kaggle/working/lstm_fold1_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 19ms/step - accuracy: 0.9431 - auc: 0.9399 - loss: 0.1653 - precision: 0.8724 - recall: 0.7133 - val_accuracy: 0.9824 - val_auc: 0.9761 - val_loss: 0.0733 - val_precision: 0.9887 - val_recall: 0.8927 - learning_rate: 0.0010\nEpoch 2/20\n\u001b[1m4685/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9815 - auc: 0.9731 - loss: 0.0759 - precision: 0.9752 - recall: 0.8994\nEpoch 2: val_auc improved from 0.97610 to 0.98037, saving model to /kaggle/working/lstm_fold1_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9815 - auc: 0.9731 - loss: 0.0759 - precision: 0.9753 - recall: 0.8994 - val_accuracy: 0.9861 - val_auc: 0.9804 - val_loss: 0.0607 - val_precision: 0.9899 - val_recall: 0.9160 - learning_rate: 0.0010\nEpoch 3/20\n\u001b[1m4685/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9848 - auc: 0.9759 - loss: 0.0662 - precision: 0.9855 - recall: 0.9120\nEpoch 3: val_auc improved from 0.98037 to 0.98131, saving model to /kaggle/working/lstm_fold1_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9848 - auc: 0.9759 - loss: 0.0662 - precision: 0.9855 - recall: 0.9120 - val_accuracy: 0.9870 - val_auc: 0.9813 - val_loss: 0.0565 - val_precision: 0.9925 - val_recall: 0.9198 - learning_rate: 0.0010\nEpoch 4/20\n\u001b[1m4685/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9855 - auc: 0.9780 - loss: 0.0632 - precision: 0.9881 - recall: 0.9140\nEpoch 4: val_auc improved from 0.98131 to 0.98170, saving model to /kaggle/working/lstm_fold1_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9855 - auc: 0.9780 - loss: 0.0632 - precision: 0.9881 - recall: 0.9140 - val_accuracy: 0.9872 - val_auc: 0.9817 - val_loss: 0.0558 - val_precision: 0.9924 - val_recall: 0.9213 - learning_rate: 0.0010\nEpoch 5/20\n\u001b[1m4685/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9860 - auc: 0.9793 - loss: 0.0607 - precision: 0.9896 - recall: 0.9156\nEpoch 5: val_auc improved from 0.98170 to 0.98246, saving model to /kaggle/working/lstm_fold1_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9860 - auc: 0.9793 - loss: 0.0607 - precision: 0.9896 - recall: 0.9156 - val_accuracy: 0.9872 - val_auc: 0.9825 - val_loss: 0.0565 - val_precision: 0.9909 - val_recall: 0.9226 - learning_rate: 0.0010\nEpoch 6/20\n\u001b[1m4686/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9865 - auc: 0.9802 - loss: 0.0590 - precision: 0.9912 - recall: 0.9175\nEpoch 6: val_auc improved from 0.98246 to 0.98395, saving model to /kaggle/working/lstm_fold1_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9865 - auc: 0.9802 - loss: 0.0590 - precision: 0.9912 - recall: 0.9175 - val_accuracy: 0.9875 - val_auc: 0.9839 - val_loss: 0.0531 - val_precision: 0.9949 - val_recall: 0.9213 - learning_rate: 0.0010\nEpoch 7/20\n\u001b[1m4686/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9867 - auc: 0.9811 - loss: 0.0578 - precision: 0.9917 - recall: 0.9187\nEpoch 7: val_auc improved from 0.98395 to 0.98425, saving model to /kaggle/working/lstm_fold1_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9867 - auc: 0.9811 - loss: 0.0578 - precision: 0.9917 - recall: 0.9187 - val_accuracy: 0.9875 - val_auc: 0.9842 - val_loss: 0.0535 - val_precision: 0.9932 - val_recall: 0.9225 - learning_rate: 0.0010\nEpoch 8/20\n\u001b[1m4687/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9869 - auc: 0.9818 - loss: 0.0567 - precision: 0.9922 - recall: 0.9193\nEpoch 8: val_auc improved from 0.98425 to 0.98530, saving model to /kaggle/working/lstm_fold1_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9869 - auc: 0.9818 - loss: 0.0567 - precision: 0.9922 - recall: 0.9193 - val_accuracy: 0.9874 - val_auc: 0.9853 - val_loss: 0.0538 - val_precision: 0.9952 - val_recall: 0.9203 - learning_rate: 0.0010\nEpoch 9/20\n\u001b[1m4686/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9869 - auc: 0.9831 - loss: 0.0559 - precision: 0.9925 - recall: 0.9195\nEpoch 9: val_auc improved from 0.98530 to 0.98780, saving model to /kaggle/working/lstm_fold1_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9869 - auc: 0.9831 - loss: 0.0559 - precision: 0.9925 - recall: 0.9195 - val_accuracy: 0.9879 - val_auc: 0.9878 - val_loss: 0.0497 - val_precision: 0.9957 - val_recall: 0.9227 - learning_rate: 0.0010\nEpoch 10/20\n\u001b[1m4686/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9869 - auc: 0.9864 - loss: 0.0543 - precision: 0.9928 - recall: 0.9192\nEpoch 10: val_auc improved from 0.98780 to 0.98891, saving model to /kaggle/working/lstm_fold1_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9869 - auc: 0.9864 - loss: 0.0543 - precision: 0.9928 - recall: 0.9192 - val_accuracy: 0.9876 - val_auc: 0.9889 - val_loss: 0.0498 - val_precision: 0.9950 - val_recall: 0.9219 - learning_rate: 0.0010\nEpoch 11/20\n\u001b[1m4686/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9873 - auc: 0.9875 - loss: 0.0523 - precision: 0.9929 - recall: 0.9215\nEpoch 11: val_auc improved from 0.98891 to 0.99000, saving model to /kaggle/working/lstm_fold1_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9873 - auc: 0.9875 - loss: 0.0523 - precision: 0.9929 - recall: 0.9215 - val_accuracy: 0.9882 - val_auc: 0.9900 - val_loss: 0.0474 - val_precision: 0.9966 - val_recall: 0.9242 - learning_rate: 0.0010\nEpoch 12/20\n\u001b[1m4685/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9875 - auc: 0.9883 - loss: 0.0510 - precision: 0.9939 - recall: 0.9220\nEpoch 12: val_auc improved from 0.99000 to 0.99021, saving model to /kaggle/working/lstm_fold1_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9875 - auc: 0.9883 - loss: 0.0510 - precision: 0.9939 - recall: 0.9220 - val_accuracy: 0.9883 - val_auc: 0.9902 - val_loss: 0.0472 - val_precision: 0.9963 - val_recall: 0.9252 - learning_rate: 0.0010\nEpoch 13/20\n\u001b[1m4686/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9877 - auc: 0.9887 - loss: 0.0500 - precision: 0.9941 - recall: 0.9232\nEpoch 13: val_auc did not improve from 0.99021\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9877 - auc: 0.9887 - loss: 0.0500 - precision: 0.9941 - recall: 0.9232 - val_accuracy: 0.9880 - val_auc: 0.9899 - val_loss: 0.0473 - val_precision: 0.9968 - val_recall: 0.9230 - learning_rate: 0.0010\nEpoch 14/20\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9876 - auc: 0.9889 - loss: 0.0503 - precision: 0.9944 - recall: 0.9222\nEpoch 14: val_auc improved from 0.99021 to 0.99064, saving model to /kaggle/working/lstm_fold1_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9876 - auc: 0.9889 - loss: 0.0503 - precision: 0.9944 - recall: 0.9222 - val_accuracy: 0.9882 - val_auc: 0.9906 - val_loss: 0.0474 - val_precision: 0.9954 - val_recall: 0.9251 - learning_rate: 0.0010\nEpoch 15/20\n\u001b[1m4685/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9877 - auc: 0.9892 - loss: 0.0498 - precision: 0.9943 - recall: 0.9229\nEpoch 15: val_auc improved from 0.99064 to 0.99087, saving model to /kaggle/working/lstm_fold1_best.keras\n\nEpoch 15: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9877 - auc: 0.9892 - loss: 0.0498 - precision: 0.9943 - recall: 0.9229 - val_accuracy: 0.9882 - val_auc: 0.9909 - val_loss: 0.0473 - val_precision: 0.9953 - val_recall: 0.9256 - learning_rate: 0.0010\nEpoch 16/20\n\u001b[1m4687/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9879 - auc: 0.9899 - loss: 0.0480 - precision: 0.9949 - recall: 0.9237\nEpoch 16: val_auc improved from 0.99087 to 0.99109, saving model to /kaggle/working/lstm_fold1_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9879 - auc: 0.9899 - loss: 0.0480 - precision: 0.9949 - recall: 0.9237 - val_accuracy: 0.9885 - val_auc: 0.9911 - val_loss: 0.0454 - val_precision: 0.9966 - val_recall: 0.9261 - learning_rate: 5.0000e-04\nEpoch 17/20\n\u001b[1m4685/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9880 - auc: 0.9901 - loss: 0.0478 - precision: 0.9950 - recall: 0.9239\nEpoch 17: val_auc improved from 0.99109 to 0.99154, saving model to /kaggle/working/lstm_fold1_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 19ms/step - accuracy: 0.9880 - auc: 0.9901 - loss: 0.0478 - precision: 0.9950 - recall: 0.9239 - val_accuracy: 0.9885 - val_auc: 0.9915 - val_loss: 0.0450 - val_precision: 0.9971 - val_recall: 0.9256 - learning_rate: 5.0000e-04\nEpoch 18/20\n\u001b[1m4687/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9880 - auc: 0.9903 - loss: 0.0473 - precision: 0.9956 - recall: 0.9241\nEpoch 18: val_auc did not improve from 0.99154\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9880 - auc: 0.9903 - loss: 0.0473 - precision: 0.9956 - recall: 0.9241 - val_accuracy: 0.9885 - val_auc: 0.9913 - val_loss: 0.0448 - val_precision: 0.9972 - val_recall: 0.9255 - learning_rate: 5.0000e-04\nEpoch 19/20\n\u001b[1m4687/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9881 - auc: 0.9905 - loss: 0.0470 - precision: 0.9956 - recall: 0.9246\nEpoch 19: val_auc did not improve from 0.99154\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9881 - auc: 0.9905 - loss: 0.0470 - precision: 0.9956 - recall: 0.9246 - val_accuracy: 0.9885 - val_auc: 0.9909 - val_loss: 0.0453 - val_precision: 0.9970 - val_recall: 0.9255 - learning_rate: 5.0000e-04\nEpoch 20/20\n\u001b[1m4687/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9882 - auc: 0.9906 - loss: 0.0468 - precision: 0.9957 - recall: 0.9251\nEpoch 20: val_auc improved from 0.99154 to 0.99188, saving model to /kaggle/working/lstm_fold1_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9882 - auc: 0.9906 - loss: 0.0468 - precision: 0.9957 - recall: 0.9251 - val_accuracy: 0.9885 - val_auc: 0.9919 - val_loss: 0.0446 - val_precision: 0.9970 - val_recall: 0.9256 - learning_rate: 5.0000e-04\nRestoring model weights from the end of the best epoch: 20.\n\nEvaluating Fold 1...\n\n======================================================================\nFOLD 1 RESULTS:\n======================================================================\n  Loss:      0.0446\n  Accuracy:  0.9885\n  Precision: 0.9970\n  Recall:    0.9256\n  F1-score:  0.9600\n  AUC:       0.9919\n======================================================================\n\n======================================================================\nTRAINING LSTM - FOLD 2/5\n======================================================================\n\nLoading data from: /kaggle/input/csecicids2018-splited/folds/fold_2\n\nData shapes:\n  X_train: (2399995, 77, 1), y_train: (2399995,)\n  X_test:  (599999, 77, 1), y_test:  (599999,)\n\nClass distribution:\n  Train - Benign: 2,041,321 (85.06%)\n  Train - Attack: 358,674 (14.94%)\n  Test  - Benign: 510,331 (85.06%)\n  Test  - Attack: 89,668 (14.94%)\n\nBuilding LSTM model...\n\nModel Summary:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m77\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚        \u001b[38;5;34m16,896\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m77\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚           \u001b[38;5;34m256\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m77\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚        \u001b[38;5;34m12,416\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_1           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚           \u001b[38;5;34m128\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m2,112\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m65\u001b[0m â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">77</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">77</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">77</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_1           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m31,873\u001b[0m (124.50 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,873</span> (124.50 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m31,681\u001b[0m (123.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,681</span> (123.75 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\nStarting training for Fold 2...\nEpoch 1/20\n\u001b[1m4686/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9429 - auc: 0.9406 - loss: 0.1655 - precision: 0.8714 - recall: 0.7196\nEpoch 1: val_auc improved from -inf to 0.97595, saving model to /kaggle/working/lstm_fold2_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 19ms/step - accuracy: 0.9429 - auc: 0.9407 - loss: 0.1655 - precision: 0.8714 - recall: 0.7197 - val_accuracy: 0.9759 - val_auc: 0.9759 - val_loss: 0.0803 - val_precision: 0.9189 - val_recall: 0.9202 - learning_rate: 0.0010\nEpoch 2/20\n\u001b[1m4686/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9814 - auc: 0.9733 - loss: 0.0771 - precision: 0.9726 - recall: 0.9009\nEpoch 2: val_auc improved from 0.97595 to 0.97780, saving model to /kaggle/working/lstm_fold2_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9814 - auc: 0.9733 - loss: 0.0771 - precision: 0.9727 - recall: 0.9009 - val_accuracy: 0.9828 - val_auc: 0.9778 - val_loss: 0.0711 - val_precision: 0.9678 - val_recall: 0.9154 - learning_rate: 0.0010\nEpoch 3/20\n\u001b[1m4687/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9849 - auc: 0.9771 - loss: 0.0656 - precision: 0.9860 - recall: 0.9120\nEpoch 3: val_auc improved from 0.97780 to 0.98109, saving model to /kaggle/working/lstm_fold2_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9849 - auc: 0.9771 - loss: 0.0656 - precision: 0.9860 - recall: 0.9120 - val_accuracy: 0.9868 - val_auc: 0.9811 - val_loss: 0.0572 - val_precision: 0.9911 - val_recall: 0.9200 - learning_rate: 0.0010\nEpoch 4/20\n\u001b[1m4685/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9858 - auc: 0.9791 - loss: 0.0615 - precision: 0.9891 - recall: 0.9152\nEpoch 4: val_auc improved from 0.98109 to 0.98155, saving model to /kaggle/working/lstm_fold2_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9858 - auc: 0.9791 - loss: 0.0615 - precision: 0.9891 - recall: 0.9152 - val_accuracy: 0.9866 - val_auc: 0.9815 - val_loss: 0.0565 - val_precision: 0.9941 - val_recall: 0.9155 - learning_rate: 0.0010\nEpoch 5/20\n\u001b[1m4686/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9863 - auc: 0.9800 - loss: 0.0592 - precision: 0.9909 - recall: 0.9168\nEpoch 5: val_auc improved from 0.98155 to 0.98272, saving model to /kaggle/working/lstm_fold2_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9863 - auc: 0.9800 - loss: 0.0592 - precision: 0.9909 - recall: 0.9168 - val_accuracy: 0.9873 - val_auc: 0.9827 - val_loss: 0.0550 - val_precision: 0.9939 - val_recall: 0.9206 - learning_rate: 0.0010\nEpoch 6/20\n\u001b[1m4685/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9867 - auc: 0.9808 - loss: 0.0575 - precision: 0.9906 - recall: 0.9196\nEpoch 6: val_auc improved from 0.98272 to 0.98339, saving model to /kaggle/working/lstm_fold2_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9867 - auc: 0.9808 - loss: 0.0575 - precision: 0.9906 - recall: 0.9196 - val_accuracy: 0.9880 - val_auc: 0.9834 - val_loss: 0.0523 - val_precision: 0.9962 - val_recall: 0.9233 - learning_rate: 0.0010\nEpoch 7/20\n\u001b[1m4685/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9872 - auc: 0.9817 - loss: 0.0557 - precision: 0.9923 - recall: 0.9212\nEpoch 7: val_auc improved from 0.98339 to 0.98339, saving model to /kaggle/working/lstm_fold2_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9872 - auc: 0.9817 - loss: 0.0557 - precision: 0.9923 - recall: 0.9212 - val_accuracy: 0.9877 - val_auc: 0.9834 - val_loss: 0.0533 - val_precision: 0.9931 - val_recall: 0.9240 - learning_rate: 0.0010\nEpoch 8/20\n\u001b[1m4686/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9873 - auc: 0.9821 - loss: 0.0551 - precision: 0.9928 - recall: 0.9222\nEpoch 8: val_auc improved from 0.98339 to 0.98418, saving model to /kaggle/working/lstm_fold2_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9873 - auc: 0.9821 - loss: 0.0551 - precision: 0.9928 - recall: 0.9222 - val_accuracy: 0.9879 - val_auc: 0.9842 - val_loss: 0.0524 - val_precision: 0.9951 - val_recall: 0.9235 - learning_rate: 0.0010\nEpoch 9/20\n\u001b[1m4685/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9875 - auc: 0.9826 - loss: 0.0545 - precision: 0.9931 - recall: 0.9224\nEpoch 9: val_auc improved from 0.98418 to 0.98862, saving model to /kaggle/working/lstm_fold2_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9875 - auc: 0.9826 - loss: 0.0545 - precision: 0.9931 - recall: 0.9224 - val_accuracy: 0.9881 - val_auc: 0.9886 - val_loss: 0.0494 - val_precision: 0.9962 - val_recall: 0.9239 - learning_rate: 0.0010\nEpoch 10/20\n\u001b[1m4686/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9874 - auc: 0.9867 - loss: 0.0530 - precision: 0.9928 - recall: 0.9224\nEpoch 10: val_auc improved from 0.98862 to 0.98895, saving model to /kaggle/working/lstm_fold2_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9874 - auc: 0.9867 - loss: 0.0530 - precision: 0.9928 - recall: 0.9224 - val_accuracy: 0.9881 - val_auc: 0.9890 - val_loss: 0.0488 - val_precision: 0.9963 - val_recall: 0.9240 - learning_rate: 0.0010\nEpoch 11/20\n\u001b[1m4685/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9876 - auc: 0.9883 - loss: 0.0507 - precision: 0.9935 - recall: 0.9231\nEpoch 11: val_auc improved from 0.98895 to 0.99013, saving model to /kaggle/working/lstm_fold2_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9876 - auc: 0.9883 - loss: 0.0507 - precision: 0.9935 - recall: 0.9231 - val_accuracy: 0.9882 - val_auc: 0.9901 - val_loss: 0.0474 - val_precision: 0.9973 - val_recall: 0.9237 - learning_rate: 0.0010\nEpoch 12/20\n\u001b[1m4685/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9877 - auc: 0.9886 - loss: 0.0504 - precision: 0.9935 - recall: 0.9237\nEpoch 12: val_auc did not improve from 0.99013\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9877 - auc: 0.9886 - loss: 0.0504 - precision: 0.9935 - recall: 0.9237 - val_accuracy: 0.9880 - val_auc: 0.9898 - val_loss: 0.0489 - val_precision: 0.9954 - val_recall: 0.9241 - learning_rate: 0.0010\nEpoch 13/20\n\u001b[1m4686/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9876 - auc: 0.9889 - loss: 0.0504 - precision: 0.9932 - recall: 0.9232\nEpoch 13: val_auc did not improve from 0.99013\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9876 - auc: 0.9889 - loss: 0.0504 - precision: 0.9932 - recall: 0.9232 - val_accuracy: 0.9881 - val_auc: 0.9901 - val_loss: 0.0482 - val_precision: 0.9959 - val_recall: 0.9244 - learning_rate: 0.0010\nEpoch 14/20\n\u001b[1m4687/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9878 - auc: 0.9892 - loss: 0.0497 - precision: 0.9940 - recall: 0.9236\nEpoch 14: val_auc improved from 0.99013 to 0.99059, saving model to /kaggle/working/lstm_fold2_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9878 - auc: 0.9892 - loss: 0.0497 - precision: 0.9940 - recall: 0.9236 - val_accuracy: 0.9883 - val_auc: 0.9906 - val_loss: 0.0466 - val_precision: 0.9969 - val_recall: 0.9245 - learning_rate: 0.0010\nEpoch 15/20\n\u001b[1m4687/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9879 - auc: 0.9899 - loss: 0.0486 - precision: 0.9937 - recall: 0.9250\nEpoch 15: val_auc did not improve from 0.99059\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9879 - auc: 0.9899 - loss: 0.0486 - precision: 0.9937 - recall: 0.9250 - val_accuracy: 0.9882 - val_auc: 0.9903 - val_loss: 0.0473 - val_precision: 0.9971 - val_recall: 0.9239 - learning_rate: 0.0010\nEpoch 16/20\n\u001b[1m4685/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9878 - auc: 0.9900 - loss: 0.0486 - precision: 0.9937 - recall: 0.9240\nEpoch 16: val_auc improved from 0.99059 to 0.99090, saving model to /kaggle/working/lstm_fold2_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9878 - auc: 0.9900 - loss: 0.0486 - precision: 0.9937 - recall: 0.9240 - val_accuracy: 0.9883 - val_auc: 0.9909 - val_loss: 0.0455 - val_precision: 0.9975 - val_recall: 0.9239 - learning_rate: 0.0010\nEpoch 17/20\n\u001b[1m4686/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9878 - auc: 0.9902 - loss: 0.0481 - precision: 0.9941 - recall: 0.9236\nEpoch 17: val_auc improved from 0.99090 to 0.99166, saving model to /kaggle/working/lstm_fold2_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9878 - auc: 0.9902 - loss: 0.0481 - precision: 0.9941 - recall: 0.9236 - val_accuracy: 0.9882 - val_auc: 0.9917 - val_loss: 0.0456 - val_precision: 0.9966 - val_recall: 0.9245 - learning_rate: 0.0010\nEpoch 18/20\n\u001b[1m4687/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9880 - auc: 0.9906 - loss: 0.0473 - precision: 0.9943 - recall: 0.9245\nEpoch 18: val_auc improved from 0.99166 to 0.99214, saving model to /kaggle/working/lstm_fold2_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9880 - auc: 0.9906 - loss: 0.0473 - precision: 0.9943 - recall: 0.9245 - val_accuracy: 0.9883 - val_auc: 0.9921 - val_loss: 0.0441 - val_precision: 0.9976 - val_recall: 0.9242 - learning_rate: 0.0010\nEpoch 19/20\n\u001b[1m4685/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9880 - auc: 0.9909 - loss: 0.0468 - precision: 0.9946 - recall: 0.9245\nEpoch 19: val_auc improved from 0.99214 to 0.99216, saving model to /kaggle/working/lstm_fold2_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9880 - auc: 0.9909 - loss: 0.0468 - precision: 0.9946 - recall: 0.9245 - val_accuracy: 0.9884 - val_auc: 0.9922 - val_loss: 0.0439 - val_precision: 0.9978 - val_recall: 0.9246 - learning_rate: 0.0010\nEpoch 20/20\n\u001b[1m4687/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9880 - auc: 0.9911 - loss: 0.0466 - precision: 0.9947 - recall: 0.9242\nEpoch 20: val_auc improved from 0.99216 to 0.99245, saving model to /kaggle/working/lstm_fold2_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 18ms/step - accuracy: 0.9880 - auc: 0.9911 - loss: 0.0466 - precision: 0.9947 - recall: 0.9242 - val_accuracy: 0.9883 - val_auc: 0.9924 - val_loss: 0.0444 - val_precision: 0.9972 - val_recall: 0.9244 - learning_rate: 0.0010\nRestoring model weights from the end of the best epoch: 19.\n\nEvaluating Fold 2...\n\n======================================================================\nFOLD 2 RESULTS:\n======================================================================\n  Loss:      0.0439\n  Accuracy:  0.9884\n  Precision: 0.9978\n  Recall:    0.9246\n  F1-score:  0.9598\n  AUC:       0.9922\n======================================================================\n\n======================================================================\nTRAINING LSTM - FOLD 3/5\n======================================================================\n\nLoading data from: /kaggle/input/csecicids2018-splited/folds/fold_3\n\nData shapes:\n  X_train: (2399995, 77, 1), y_train: (2399995,)\n  X_test:  (599999, 77, 1), y_test:  (599999,)\n\nClass distribution:\n  Train - Benign: 2,041,322 (85.06%)\n  Train - Attack: 358,673 (14.94%)\n  Test  - Benign: 510,330 (85.06%)\n  Test  - Attack: 89,669 (14.94%)\n\nBuilding LSTM model...\n\nModel Summary:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m77\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚        \u001b[38;5;34m16,896\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m77\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚           \u001b[38;5;34m256\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m77\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚        \u001b[38;5;34m12,416\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_1           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚           \u001b[38;5;34m128\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚         \u001b[38;5;34m2,112\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m65\u001b[0m â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">77</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">77</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">77</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_1           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m31,873\u001b[0m (124.50 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,873</span> (124.50 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m31,681\u001b[0m (123.75 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,681</span> (123.75 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\nStarting training for Fold 3...\nEpoch 1/20\n\u001b[1m4685/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9443 - auc: 0.9415 - loss: 0.1619 - precision: 0.8795 - recall: 0.7148\nEpoch 1: val_auc improved from -inf to 0.97554, saving model to /kaggle/working/lstm_fold3_best.keras\n\u001b[1m4688/4688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 19ms/step - accuracy: 0.9443 - auc: 0.9415 - loss: 0.1618 - precision: 0.8796 - recall: 0.7149 - val_accuracy: 0.9809 - val_auc: 0.9755 - val_loss: 0.0716 - val_precision: 0.9520 - val_recall: 0.9186 - learning_rate: 0.0010\nEpoch 2/20\n\u001b[1m1300/4688\u001b[0m \u001b[32mâ”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m57s\u001b[0m 17ms/step - accuracy: 0.9795 - auc: 0.9726 - loss: 0.0820 - precision: 0.9672 - recall: 0.8936","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow import keras\nimport pandas as pd\n\nN_FOLDS = 5\nresults = []\n\nfor fold in range(1, N_FOLDS + 1):\n    print(f\"\\n{'='*50}\")\n    print(f\"Testing Fold {fold}\")\n    print('='*50)\n    \n    # Load model\n    model = keras.models.load_model(f'/kaggle/working/lstm_fold{fold}_best.keras')\n    \n    # Load test data\n    X_test = np.load(f'/kaggle/input/csecicids2018-splited/folds/fold_{fold}/X_test.npy')\n    y_test = np.load(f'/kaggle/input/csecicids2018-splited/folds/fold_{fold}/y_test.npy')\n    \n    # Reshape\n    X_test_lstm = X_test.reshape(-1, X_test.shape[1], 1)\n    \n    # Evaluate\n    test_results = model.evaluate(X_test_lstm, y_test, verbose=0)\n    \n    results.append({\n        'fold': fold,\n        'accuracy': test_results[1],\n        'precision': test_results[2],\n        'recall': test_results[3],\n        'auc': test_results[4]\n    })\n    \n    print(f\"Accuracy:  {test_results[1]:.4f}\")\n    print(f\"Precision: {test_results[2]:.4f}\")\n    print(f\"Recall:    {test_results[3]:.4f}\")\n    print(f\"AUC:       {test_results[4]:.4f}\")\n\n# Tá»•ng há»£p\ndf = pd.DataFrame(results)\nprint(\"\\n\" + \"=\"*50)\nprint(\"FINAL RESULTS (MEAN Â± STD):\")\nprint(\"=\"*50)\nfor col in ['accuracy', 'precision', 'recall', 'auc']:\n    print(f\"{col.capitalize():10s}: {df[col].mean():.4f} Â± {df[col].std():.4f}\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test_lstm_models.py\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom pathlib import Path\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Cáº¥u hÃ¬nh\nBASE_FOLD_DIR = Path('/kaggle/input/csecicids2018-splited/folds')\nMODEL_DIR = Path('/kaggle/input/model-lstm')\nN_FOLDS = 5\n\nprint(\"=\"*70)\nprint(\"TESTING LSTM MODELS ON 5-FOLD CROSS-VALIDATION\")\nprint(\"=\"*70)\n\n# LÆ°u káº¿t quáº£\ntest_results = []\n\nfor fold in range(1, N_FOLDS + 1):\n    print(f\"\\n{'='*70}\")\n    print(f\"TESTING LSTM - FOLD {fold}/{N_FOLDS}\")\n    print(f\"{'='*70}\")\n    \n    # Load model\n    model_path = MODEL_DIR / f'lstm_fold{fold}_best.keras'\n    print(f\"Loading: {model_path.name}\")\n    model = keras.models.load_model(model_path)\n    \n    # Load test data\n    X_test = np.load(BASE_FOLD_DIR / f'fold_{fold}' / 'X_test.npy')\n    y_test = np.load(BASE_FOLD_DIR / f'fold_{fold}' / 'y_test.npy')\n    \n    # Reshape cho LSTM\n    X_test_lstm = X_test.reshape(-1, X_test.shape[1], 1)\n    \n    print(f\"Test samples: {len(X_test_lstm):,}\")\n    print(f\"  Benign: {(y_test==0).sum():,} | Attack: {(y_test==1).sum():,}\")\n    \n    # Evaluate\n    test_loss, test_acc, test_prec, test_rec, test_auc = model.evaluate(\n        X_test_lstm, y_test, \n        batch_size=512,\n        verbose=0\n    )\n    \n    # Calculate F1-score\n    test_f1 = 2 * test_prec * test_rec / (test_prec + test_rec + 1e-8)\n    \n    # Predictions\n    y_pred = (model.predict(X_test_lstm, batch_size=512, verbose=0) > 0.5).astype(int).flatten()\n    \n    # Confusion Matrix\n    cm = confusion_matrix(y_test, y_pred)\n    tn, fp, fn, tp = cm.ravel()\n    \n    # Additional metrics\n    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n    \n    # Print results\n    print(f\"\\nMetrics:\")\n    print(f\"  Accuracy:    {test_acc:.4f}\")\n    print(f\"  Precision:   {test_prec:.4f}\")\n    print(f\"  Recall:      {test_rec:.4f}\")\n    print(f\"  F1-score:    {test_f1:.4f}\")\n    print(f\"  AUC:         {test_auc:.4f}\")\n    print(f\"  Specificity: {specificity:.4f}\")\n    \n    print(f\"\\nConfusion Matrix:\")\n    print(f\"  TN: {tn:>7,} | FP: {fp:>7,}\")\n    print(f\"  FN: {fn:>7,} | TP: {tp:>7,}\")\n    \n    # Store results\n    test_results.append({\n        'fold': fold,\n        'loss': test_loss,\n        'accuracy': test_acc,\n        'precision': test_prec,\n        'recall': test_rec,\n        'f1': test_f1,\n        'auc': test_auc,\n        'specificity': specificity,\n        'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp\n    })\n    \n    # Save confusion matrix\n    plt.figure(figsize=(7, 5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n                xticklabels=['Benign', 'Attack'],\n                yticklabels=['Benign', 'Attack'],\n                cbar_kws={'label': 'Count'})\n    plt.title(f'LSTM - Fold {fold} Confusion Matrix', fontsize=13, fontweight='bold')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.tight_layout()\n    plt.savefig(f'/kaggle/working/lstm_fold{fold}_cm.png', dpi=120)\n    plt.close()\n    \n    # Clean up\n    del model, X_test, y_test, X_test_lstm, y_pred\n    tf.keras.backend.clear_session()\n\n# ============================================================================\n# Tá»”NG Há»¢P Káº¾T QUáº¢\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"LSTM 5-FOLD CROSS-VALIDATION SUMMARY\")\nprint(\"=\"*70)\n\ndf = pd.DataFrame(test_results)\n\n# Hiá»ƒn thá»‹ báº£ng káº¿t quáº£\nprint(\"\\nResults per fold:\")\nprint(df[['fold', 'accuracy', 'precision', 'recall', 'f1', 'auc']].to_string(index=False))\n\n# Mean Â± STD\nprint(\"\\n\" + \"=\"*70)\nprint(\"MEAN METRICS (MEAN Â± STD):\")\nprint(\"=\"*70)\n\nmetrics = ['loss', 'accuracy', 'precision', 'recall', 'f1', 'auc', 'specificity']\nfor metric in metrics:\n    mean_val = df[metric].mean()\n    std_val = df[metric].std()\n    print(f\"  {metric.upper():12s}: {mean_val:.4f} Â± {std_val:.4f}\")\n\n# Tá»•ng confusion matrix\nprint(\"\\n\" + \"=\"*70)\nprint(\"AGGREGATED CONFUSION MATRIX:\")\nprint(\"=\"*70)\ntotal_tn = df['tn'].sum()\ntotal_fp = df['fp'].sum()\ntotal_fn = df['fn'].sum()\ntotal_tp = df['tp'].sum()\n\nprint(f\"  TN: {total_tn:>10,} | FP: {total_fp:>10,}\")\nprint(f\"  FN: {total_fn:>10,} | TP: {total_tp:>10,}\")\n\ntotal = total_tn + total_fp + total_fn + total_tp\noverall_acc = (total_tn + total_tp) / total\nprint(f\"\\nOverall Accuracy: {overall_acc:.4f}\")\n\n# LÆ°u káº¿t quáº£\ndf.to_csv('/kaggle/working/lstm_test_results.csv', index=False)\nprint(\"\\nâœ“ Results saved: lstm_test_results.csv\")\n\n# Visualization: Metrics comparison\nfig, ax = plt.subplots(figsize=(12, 6))\n\nx = np.arange(1, N_FOLDS + 1)\nwidth = 0.15\nmetrics_plot = ['accuracy', 'precision', 'recall', 'f1', 'auc']\ncolors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12', '#9b59b6']\n\nfor i, (metric, color) in enumerate(zip(metrics_plot, colors)):\n    values = df[metric].values\n    ax.bar(x + i*width, values, width, label=metric.capitalize(), color=color, alpha=0.8)\n\nax.set_xlabel('Fold', fontsize=12, fontweight='bold')\nax.set_ylabel('Score', fontsize=12, fontweight='bold')\nax.set_title('LSTM Metrics Across 5 Folds', fontsize=14, fontweight='bold')\nax.set_xticks(x + width*2)\nax.set_xticklabels([f'Fold {i}' for i in range(1, N_FOLDS+1)])\nax.legend(loc='lower right')\nax.set_ylim([0.90, 1.0])\nax.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/lstm_metrics_comparison.png', dpi=120)\nplt.close()\nprint(\"âœ“ Metrics plot saved: lstm_metrics_comparison.png\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… LSTM TESTING COMPLETED!\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:01:28.900190Z","iopub.execute_input":"2025-12-05T15:01:28.900462Z","iopub.status.idle":"2025-12-05T15:02:50.653570Z","shell.execute_reply.started":"2025-12-05T15:01:28.900442Z","shell.execute_reply":"2025-12-05T15:02:50.653005Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"======================================================================\nTESTING LSTM MODELS ON 5-FOLD CROSS-VALIDATION\n======================================================================\n\n======================================================================\nTESTING LSTM - FOLD 1/5\n======================================================================\nLoading: lstm_fold1_best.keras\nTest samples: 599,999\n  Benign: 510,331 | Attack: 89,668\n\nMetrics:\n  Accuracy:    0.9885\n  Precision:   0.9970\n  Recall:      0.9256\n  F1-score:    0.9600\n  AUC:         0.9919\n  Specificity: 0.9995\n\nConfusion Matrix:\n  TN: 510,079 | FP:     252\n  FN:   6,672 | TP:  82,996\n\n======================================================================\nTESTING LSTM - FOLD 2/5\n======================================================================\nLoading: lstm_fold2_best.keras\nTest samples: 599,999\n  Benign: 510,331 | Attack: 89,668\n\nMetrics:\n  Accuracy:    0.9883\n  Precision:   0.9972\n  Recall:      0.9244\n  F1-score:    0.9594\n  AUC:         0.9924\n  Specificity: 0.9995\n\nConfusion Matrix:\n  TN: 510,100 | FP:     231\n  FN:   6,776 | TP:  82,892\n\n======================================================================\nTESTING LSTM - FOLD 3/5\n======================================================================\nLoading: lstm_fold3_best.keras\nTest samples: 599,999\n  Benign: 510,330 | Attack: 89,669\n\nMetrics:\n  Accuracy:    0.9881\n  Precision:   0.9972\n  Recall:      0.9229\n  F1-score:    0.9586\n  AUC:         0.9920\n  Specificity: 0.9995\n\nConfusion Matrix:\n  TN: 510,100 | FP:     230\n  FN:   6,915 | TP:  82,754\n\n======================================================================\nTESTING LSTM - FOLD 4/5\n======================================================================\nLoading: lstm_fold4_best.keras\nTest samples: 599,999\n  Benign: 510,330 | Attack: 89,669\n\nMetrics:\n  Accuracy:    0.9881\n  Precision:   0.9968\n  Recall:      0.9233\n  F1-score:    0.9586\n  AUC:         0.9918\n  Specificity: 0.9995\n\nConfusion Matrix:\n  TN: 510,061 | FP:     269\n  FN:   6,874 | TP:  82,795\n\n======================================================================\nTESTING LSTM - FOLD 5/5\n======================================================================\nLoading: lstm_fold5_best.keras\nTest samples: 599,998\n  Benign: 510,330 | Attack: 89,668\n\nMetrics:\n  Accuracy:    0.9883\n  Precision:   0.9965\n  Recall:      0.9247\n  F1-score:    0.9593\n  AUC:         0.9911\n  Specificity: 0.9994\n\nConfusion Matrix:\n  TN: 510,042 | FP:     288\n  FN:   6,756 | TP:  82,912\n\n======================================================================\nLSTM 5-FOLD CROSS-VALIDATION SUMMARY\n======================================================================\n\nResults per fold:\n fold  accuracy  precision   recall       f1      auc\n    1  0.988460   0.996973 0.925592 0.959957 0.991880\n    2  0.988322   0.997221 0.924432 0.959448 0.992450\n    3  0.988092   0.997228 0.922883 0.958616 0.992016\n    4  0.988095   0.996762 0.923340 0.958647 0.991796\n    5  0.988260   0.996538 0.924655 0.959252 0.991105\n\n======================================================================\nMEAN METRICS (MEAN Â± STD):\n======================================================================\n  LOSS        : 0.0452 Â± 0.0007\n  ACCURACY    : 0.9882 Â± 0.0002\n  PRECISION   : 0.9969 Â± 0.0003\n  RECALL      : 0.9242 Â± 0.0011\n  F1          : 0.9592 Â± 0.0006\n  AUC         : 0.9918 Â± 0.0005\n  SPECIFICITY : 0.9995 Â± 0.0000\n\n======================================================================\nAGGREGATED CONFUSION MATRIX:\n======================================================================\n  TN:  2,550,382 | FP:      1,270\n  FN:     33,993 | TP:    414,349\n\nOverall Accuracy: 0.9882\n\nâœ“ Results saved: lstm_test_results.csv\nâœ“ Metrics plot saved: lstm_metrics_comparison.png\n\n======================================================================\nâœ… LSTM TESTING COMPLETED!\n======================================================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# test_cnn_models.py\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom pathlib import Path\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Cáº¥u hÃ¬nh\nBASE_FOLD_DIR = Path('/kaggle/input/csecicids2018-splited/folds')\nMODEL_DIR = Path('/kaggle/input/model-cnn')\nN_FOLDS = 5\n\nprint(\"=\"*70)\nprint(\"TESTING CNN MODELS ON 5-FOLD CROSS-VALIDATION\")\nprint(\"=\"*70)\n\n# LÆ°u káº¿t quáº£\ntest_results = []\n\nfor fold in range(1, N_FOLDS + 1):\n    print(f\"\\n{'='*70}\")\n    print(f\"TESTING CNN - FOLD {fold}/{N_FOLDS}\")\n    print(f\"{'='*70}\")\n    \n    # Load model\n    model_path = MODEL_DIR / f'cnn_fold{fold}_best.keras'\n    print(f\"Loading: {model_path.name}\")\n    model = keras.models.load_model(model_path)\n    \n    # Load test data\n    X_test = np.load(BASE_FOLD_DIR / f'fold_{fold}' / 'X_test.npy')\n    y_test = np.load(BASE_FOLD_DIR / f'fold_{fold}' / 'y_test.npy')\n    \n    # Reshape cho CNN\n    X_test_cnn = X_test.reshape(-1, X_test.shape[1], 1)\n    \n    print(f\"Test samples: {len(X_test_cnn):,}\")\n    print(f\"  Benign: {(y_test==0).sum():,} | Attack: {(y_test==1).sum():,}\")\n    \n    # Evaluate\n    test_loss, test_acc, test_prec, test_rec = model.evaluate(\n        X_test_cnn, y_test,\n        batch_size=512,\n        verbose=0\n    )\n    \n    # Calculate F1-score\n    test_f1 = 2 * test_prec * test_rec / (test_prec + test_rec + 1e-8)\n    \n    # Predictions\n    y_pred = (model.predict(X_test_cnn, batch_size=512, verbose=0) > 0.5).astype(int).flatten()\n    \n    # Confusion Matrix\n    cm = confusion_matrix(y_test, y_pred)\n    tn, fp, fn, tp = cm.ravel()\n    \n    # Additional metrics\n    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n    \n    # Print results\n    print(f\"\\nMetrics:\")\n    print(f\"  Accuracy:    {test_acc:.4f}\")\n    print(f\"  Precision:   {test_prec:.4f}\")\n    print(f\"  Recall:      {test_rec:.4f}\")\n    print(f\"  F1-score:    {test_f1:.4f}\")\n    print(f\"  Specificity: {specificity:.4f}\")\n    \n    print(f\"\\nConfusion Matrix:\")\n    print(f\"  TN: {tn:>7,} | FP: {fp:>7,}\")\n    print(f\"  FN: {fn:>7,} | TP: {tp:>7,}\")\n    \n    # Store results\n    test_results.append({\n        'fold': fold,\n        'loss': test_loss,\n        'accuracy': test_acc,\n        'precision': test_prec,\n        'recall': test_rec,\n        'f1': test_f1,\n        'specificity': specificity,\n        'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp\n    })\n    \n    # Save confusion matrix\n    plt.figure(figsize=(7, 5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n                xticklabels=['Benign', 'Attack'],\n                yticklabels=['Benign', 'Attack'],\n                cbar_kws={'label': 'Count'})\n    plt.title(f'CNN - Fold {fold} Confusion Matrix', fontsize=13, fontweight='bold')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.tight_layout()\n    plt.savefig(f'/kaggle/working/cnn_fold{fold}_cm.png', dpi=120)\n    plt.close()\n    \n    # Clean up\n    del model, X_test, y_test, X_test_cnn, y_pred\n    tf.keras.backend.clear_session()\n\n# ============================================================================\n# Tá»”NG Há»¢P Káº¾T QUáº¢\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"CNN 5-FOLD CROSS-VALIDATION SUMMARY\")\nprint(\"=\"*70)\n\ndf = pd.DataFrame(test_results)\n\n# Hiá»ƒn thá»‹ báº£ng káº¿t quáº£\nprint(\"\\nResults per fold:\")\nprint(df[['fold', 'accuracy', 'precision', 'recall', 'f1']].to_string(index=False))\n\n# Mean Â± STD\nprint(\"\\n\" + \"=\"*70)\nprint(\"MEAN METRICS (MEAN Â± STD):\")\nprint(\"=\"*70)\n\nmetrics = ['loss', 'accuracy', 'precision', 'recall', 'f1', 'specificity']\nfor metric in metrics:\n    mean_val = df[metric].mean()\n    std_val = df[metric].std()\n    print(f\"  {metric.upper():12s}: {mean_val:.4f} Â± {std_val:.4f}\")\n\n# Tá»•ng confusion matrix\nprint(\"\\n\" + \"=\"*70)\nprint(\"AGGREGATED CONFUSION MATRIX:\")\nprint(\"=\"*70)\ntotal_tn = df['tn'].sum()\ntotal_fp = df['fp'].sum()\ntotal_fn = df['fn'].sum()\ntotal_tp = df['tp'].sum()\n\nprint(f\"  TN: {total_tn:>10,} | FP: {total_fp:>10,}\")\nprint(f\"  FN: {total_fn:>10,} | TP: {total_tp:>10,}\")\n\ntotal = total_tn + total_fp + total_fn + total_tp\noverall_acc = (total_tn + total_tp) / total\nprint(f\"\\nOverall Accuracy: {overall_acc:.4f}\")\n\n# LÆ°u káº¿t quáº£\ndf.to_csv('/kaggle/working/cnn_test_results.csv', index=False)\nprint(\"\\nâœ“ Results saved: cnn_test_results.csv\")\n\n# Visualization: Metrics comparison\nfig, ax = plt.subplots(figsize=(12, 6))\n\nx = np.arange(1, N_FOLDS + 1)\nwidth = 0.18\nmetrics_plot = ['accuracy', 'precision', 'recall', 'f1']\ncolors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\n\nfor i, (metric, color) in enumerate(zip(metrics_plot, colors)):\n    values = df[metric].values\n    ax.bar(x + i*width, values, width, label=metric.capitalize(), color=color, alpha=0.8)\n\nax.set_xlabel('Fold', fontsize=12, fontweight='bold')\nax.set_ylabel('Score', fontsize=12, fontweight='bold')\nax.set_title('CNN Metrics Across 5 Folds', fontsize=14, fontweight='bold')\nax.set_xticks(x + width*1.5)\nax.set_xticklabels([f'Fold {i}' for i in range(1, N_FOLDS+1)])\nax.legend(loc='lower right')\nax.set_ylim([0.90, 1.0])\nax.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/cnn_metrics_comparison.png', dpi=120)\nplt.close()\nprint(\"âœ“ Metrics plot saved: cnn_metrics_comparison.png\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"âœ… CNN TESTING COMPLETED!\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T15:00:19.572552Z","iopub.execute_input":"2025-12-05T15:00:19.573180Z","iopub.status.idle":"2025-12-05T15:01:14.280884Z","shell.execute_reply.started":"2025-12-05T15:00:19.573153Z","shell.execute_reply":"2025-12-05T15:01:14.280277Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"======================================================================\nTESTING CNN MODELS ON 5-FOLD CROSS-VALIDATION\n======================================================================\n\n======================================================================\nTESTING CNN - FOLD 1/5\n======================================================================\nLoading: cnn_fold1_best.keras\nTest samples: 599,999\n  Benign: 510,331 | Attack: 89,668\n\nMetrics:\n  Accuracy:    0.9885\n  Precision:   0.9952\n  Recall:      0.9277\n  F1-score:    0.9603\n  Specificity: 0.9992\n\nConfusion Matrix:\n  TN: 509,933 | FP:     398\n  FN:   6,484 | TP:  83,184\n\n======================================================================\nTESTING CNN - FOLD 2/5\n======================================================================\nLoading: cnn_fold2_best.keras\nTest samples: 599,999\n  Benign: 510,331 | Attack: 89,668\n\nMetrics:\n  Accuracy:    0.9884\n  Precision:   0.9947\n  Recall:      0.9273\n  F1-score:    0.9598\n  Specificity: 0.9991\n\nConfusion Matrix:\n  TN: 509,886 | FP:     445\n  FN:   6,521 | TP:  83,147\n\n======================================================================\nTESTING CNN - FOLD 3/5\n======================================================================\nLoading: cnn_fold3_best.keras\nTest samples: 599,999\n  Benign: 510,330 | Attack: 89,669\n\nMetrics:\n  Accuracy:    0.9881\n  Precision:   0.9956\n  Recall:      0.9247\n  F1-score:    0.9588\n  Specificity: 0.9993\n\nConfusion Matrix:\n  TN: 509,963 | FP:     367\n  FN:   6,753 | TP:  82,916\n\n======================================================================\nTESTING CNN - FOLD 4/5\n======================================================================\nLoading: cnn_fold4_best.keras\nTest samples: 599,999\n  Benign: 510,330 | Attack: 89,669\n\nMetrics:\n  Accuracy:    0.9883\n  Precision:   0.9953\n  Recall:      0.9258\n  F1-score:    0.9593\n  Specificity: 0.9992\n\nConfusion Matrix:\n  TN: 509,940 | FP:     390\n  FN:   6,653 | TP:  83,016\n\n======================================================================\nTESTING CNN - FOLD 5/5\n======================================================================\nLoading: cnn_fold5_best.keras\nTest samples: 599,998\n  Benign: 510,330 | Attack: 89,668\n\nMetrics:\n  Accuracy:    0.9885\n  Precision:   0.9957\n  Recall:      0.9270\n  F1-score:    0.9601\n  Specificity: 0.9993\n\nConfusion Matrix:\n  TN: 509,975 | FP:     355\n  FN:   6,548 | TP:  83,120\n\n======================================================================\nCNN 5-FOLD CROSS-VALIDATION SUMMARY\n======================================================================\n\nResults per fold:\n fold  accuracy  precision   recall       f1\n    1  0.988530   0.995238 0.927689 0.960277\n    2  0.988390   0.994677 0.927276 0.959795\n    3  0.988133   0.995593 0.924690 0.958832\n    4  0.988262   0.995324 0.925805 0.959307\n    5  0.988495   0.995747 0.926975 0.960131\n\n======================================================================\nMEAN METRICS (MEAN Â± STD):\n======================================================================\n  LOSS        : 0.0416 Â± 0.0004\n  ACCURACY    : 0.9884 Â± 0.0002\n  PRECISION   : 0.9953 Â± 0.0004\n  RECALL      : 0.9265 Â± 0.0012\n  F1          : 0.9597 Â± 0.0006\n  SPECIFICITY : 0.9992 Â± 0.0001\n\n======================================================================\nAGGREGATED CONFUSION MATRIX:\n======================================================================\n  TN:  2,549,697 | FP:      1,955\n  FN:     32,959 | TP:    415,383\n\nOverall Accuracy: 0.9884\n\nâœ“ Results saved: cnn_test_results.csv\nâœ“ Metrics plot saved: cnn_metrics_comparison.png\n\n======================================================================\nâœ… CNN TESTING COMPLETED!\n======================================================================\n","output_type":"stream"}],"execution_count":2}]}
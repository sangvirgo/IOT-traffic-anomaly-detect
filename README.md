# CICIDS2018 Data Preprocessing Pipeline

Complete pipeline for cleaning, splitting, and preparing CICIDS2018 dataset for GNN, CNN, and LSTM models.

## ğŸ“ Project Structure
venv\Scripts\activate
```
project/
â”œâ”€â”€ raw_data/                    # Raw CSV files tá»« CICIDS2018
â”‚   â”œâ”€â”€ 02-14-2018.csv
â”‚   â”œâ”€â”€ 02-15-2018.csv
â”‚   â””â”€â”€ ...
â”œâ”€â”€ cleaned_data/               # Cleaned data
â”‚   â”œâ”€â”€ for_gnn/               # Version cÃ³ IP addresses (cho GNN)
â”‚   â”‚   â”œâ”€â”€ cleaned_02-14-2018.csv
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ for_cnn_lstm/          # Version khÃ´ng cÃ³ IP (cho CNN/LSTM)
â”‚       â”œâ”€â”€ cleaned_02-14-2018.csv
â”‚       â””â”€â”€ ...
â”œâ”€â”€ split_data/                # Train/Val/Test splits
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ X_train.npy
â”‚   â”‚   â”œâ”€â”€ y_train.npy
â”‚   â”‚   â”œâ”€â”€ X_val.npy
â”‚   â”‚   â”œâ”€â”€ y_val.npy
â”‚   â”‚   â”œâ”€â”€ X_test.npy
â”‚   â”‚   â””â”€â”€ y_test.npy
â”‚   â”œâ”€â”€ scalers/
â”‚   â”‚   â”œâ”€â”€ feature_scaler.pkl
â”‚   â”‚   â””â”€â”€ feature_names.pkl
â”‚   â””â”€â”€ metadata.pkl
â”œâ”€â”€ clean_cicids.py            # Cleaning script
â”œâ”€â”€ split_data.py              # Splitting script
â””â”€â”€ utils.py                   # Inspection utilities
```

## ğŸš€ Quick Start

### Requirement
```bash
pip install pandas numpy scikit-learn imbalanced-learn matplotlib seaborn

# For deep learning:
pip install tensorflow  # For CNN/LSTM

# For GNN:
pip install torch torch-geometric
```

### Step 1: Clean Raw Data

```bash
# For both GNN and CNN/LSTM (recommended)
python clean_cicids.py \
    --input_dir ./raw_data \
    --output_dir ./cleaned_data \
    --mode both

# Only for GNN (keeps IP addresses)
python clean_cicids.py \
    --input_dir ./raw_data \
    --output_dir ./cleaned_data \
    --mode gnn

# Only for CNN/LSTM (removes IP addresses)
python ./pretrain/clean_cicids.py --input_dir ./raw_data --output_dir ./cleaned_data --mode cnn_lstm
```

**What it does:**
- âœ… Removes duplicates
- âœ… Handles missing values (NaN, Inf)
- âœ… Removes invalid flows (negative duration, etc.)
- âœ… Parses timestamps
- âœ… Maps attack labels to numeric (0-5)
- âœ… Removes low-variance features
- âœ… Handles outliers (winsorization)
- âœ… Log transforms skewed features

### Step 2: Split Data

```bash
# Temporal split (recommended for time-series data)
python ./pretrain/split_data.py --input_dir ./cleaned_data/ --output_dir ./split_data --temporal
```

**What it does:**
- âœ… Splits data 70-15-15 (train-val-test)
- âœ… Balances classes using SMOTE + Undersampling
- âœ… Normalizes features using StandardScaler
- âœ… Saves as numpy arrays for fast loading

### Step 3: Inspect Data

```bash
# Inspect cleaned files
python ./pretrain/utils.py --inspect --data_dir ./cleaned_data/

# Inspect split data
python ./pretrain/utils.py --inspect --data_dir ./split_data

# Visualize label distribution
python ./pretrain/utils.py --visualize --data_dir ./split_data

# Show feature statistics
python ./pretrain/utils.py --statistics --data_dir ./split_data
```

## ğŸ“Š Label Mapping

```python
label_mapping = {
    'Benign': 0,
    'Bot': 1,
    'DDoS': 2,              # Includes all DDoS variants
    'DoS-*': 3,             # All DoS attacks grouped
    'Brute Force': 4,       # FTP, SSH, Web, XSS, SQL Injection
    'Infiltration': 5,
}
```

**Äáº·c Ä‘iá»ƒm:**
- Class 0 (Benign): Traffic bÃ¬nh thÆ°á»ng
- Class 1 (Bot): Botnet traffic
- Class 2 (DDoS): Distributed Denial of Service
- Class 3 (DoS): Denial of Service (single source)
- Class 4 (Brute Force): CÃ¡c attack brute force
- Class 5 (Infiltration): Infiltration attacks

## ğŸ’» Load Data for Training

### For CNN/LSTM

```python
import numpy as np
from utils import DataLoader

# Load data
(X_train, y_train), (X_val, y_val), (X_test, y_test) = \
    DataLoader.load_data('./split_data')

# Load metadata
metadata = DataLoader.load_metadata('./split_data')
print(f"Number of features: {metadata['n_features']}")
print(f"Number of classes: {metadata['n_classes']}")

# Reshape for CNN (example: 1D CNN)
X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_val_cnn = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)
X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

# Reshape for LSTM (example: sequence of 10 timesteps)
timesteps = 10
X_train_lstm = X_train.reshape(X_train.shape[0], timesteps, -1)
X_val_lstm = X_val.reshape(X_val.shape[0], timesteps, -1)
X_test_lstm = X_test.reshape(X_test.shape[0], timesteps, -1)
```

### For GNN

```python
import pandas as pd
import torch
from torch_geometric.data import Data

# Load cleaned data with IP addresses
df = pd.read_csv('./cleaned_data/for_gnn/cleaned_02-14-2018.csv')

# Create nodes (unique IPs)
all_ips = pd.concat([df['Src_IP'], df['Dst_IP']]).unique()
ip_to_idx = {ip: idx for idx, ip in enumerate(all_ips)}

# Create edges
edge_index = torch.tensor([
    [ip_to_idx[src] for src in df['Src_IP']],
    [ip_to_idx[dst] for dst in df['Dst_IP']]
], dtype=torch.long)

# Node features (aggregate per IP)
node_features = []  # Aggregate flow stats per IP
node_labels = []    # Label per IP

# Create PyG Data object
data = Data(
    x=torch.tensor(node_features, dtype=torch.float),
    edge_index=edge_index,
    y=torch.tensor(node_labels, dtype=torch.long)
)
```

## ğŸ“ˆ Expected Data Sizes

### CICIDS2018 Full Dataset
- **Raw**: ~15 GB (10 CSV files)
- **Cleaned**: ~10 GB
- **Split numpy**: ~2-3 GB

### After Class Balancing
- Training set: ~1-2M samples (balanced)
- Validation set: ~200-300K samples
- Test set: ~200-300K samples

## âš™ï¸ Configuration Options

### Cleaning Parameters

```python
# In clean_cicids.py

# Label mapping (customize for your needs)
label_mapping = {
    'Benign': 0,
    'Bot': 1,
    'DDoS': 2,
    # Add more...
}

# Features to remove
features_to_remove = [
    'Fwd Byts/b Avg',
    'Fwd Pkts/b Avg',
    # Add more...
]

# Outlier handling (percentiles)
lower_percentile = 0.01  # 1st percentile
upper_percentile = 0.99  # 99th percentile
```

### Splitting Parameters

```python
# In split_data.py

# Split ratios
train_ratio = 0.7   # 70% train
val_ratio = 0.15    # 15% validation
test_ratio = 0.15   # 15% test

# Class balancing strategy
over_sampling = SMOTE(sampling_strategy='auto')
under_sampling = RandomUnderSampler(sampling_strategy='auto')
```

## ğŸ” Troubleshooting

### Problem: Out of Memory

**Solution 1**: Process files one by one
```python
# Modify clean_cicids.py
chunksize = 50000  # Reduce chunk size
```

**Solution 2**: Sample data
```python
# After loading
df = df.sample(frac=0.5, random_state=42)  # Use 50% of data
```

### Problem: Class Imbalance Still High

**Solution**: Adjust sampling strategy
```python
# In split_data.py
# Manual sampling strategy
sampling_strategy = {
    0: 100000,  # Benign: reduce to 100k
    1: 50000,   # Bot: oversample to 50k
    2: 50000,   # DDoS: oversample to 50k
    # etc.
}
over = SMOTE(sampling_strategy=sampling_strategy)
```

### Problem: Features Not Scaling Properly

**Solution**: Check for constant features
```python
# Remove constant or near-constant features
from sklearn.feature_selection import VarianceThreshold

selector = VarianceThreshold(threshold=0.01)
X_train_filtered = selector.fit_transform(X_train)
```



CICIDS2018 Dataset: https://www.unb.ca/cic/datasets/ids-2018.html


## âœ… Checklist

- [ ] Downloaded CICIDS2018 raw data
- [ ] Cleaned data (both GNN and CNN/LSTM versions)
- [ ] Split data (train/val/test)
- [ ] Inspected data quality
- [ ] Implemented CNN model
- [ ] Implemented LSTM model
- [ ] Implemented GNN model
- [ ] Compared model performances
- [ ] Wrote report

## ğŸ“ Support

Náº¿u gáº·p lá»—i, check:
1. File paths Ä‘Ãºng chÆ°a
2. Memory Ä‘á»§ khÃ´ng (recommend 16GB RAM)
3. Dependencies Ä‘Ã£ cÃ i Ä‘á»§ chÆ°a
4. Data format Ä‘Ãºng chÆ°a (CSV with headers)
